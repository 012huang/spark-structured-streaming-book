== [[flatMapGroupsWithState]] flatMapGroupsWithState Operator -- Arbitrary Stateful Stream Aggregation

[source, scala]
----
flatMapGroupsWithState[S: Encoder, U: Encoder](
  outputMode: OutputMode,
  timeoutConf: GroupStateTimeout)(
  func: (K, Iterator[V], GroupState[S]) => Iterator[U]): Dataset[U]
----

NOTE: `flatMapGroupsWithState` requires link:link:spark-sql-streaming-OutputMode.adoc#Append[Append] or link:link:spark-sql-streaming-OutputMode.adoc#Update[Update] output modes.

NOTE: Every time the state function `func` is executed for a key, the state (as `GroupState[S]`) is for this key only.

CAUTION: FIXME Why can't `flatMapGroupsWithState` work with Complete output mode?

[NOTE]
====
* `K` is the type of the keys in `KeyValueGroupedDataset`

* `V` is the type of the values (per key) in `KeyValueGroupedDataset`

* `S` is the user-defined type of the state as maintained for each group

* `U` is the type of rows in the result `Dataset`
====

[source, scala]
----
scala> spark.version
res0: String = 2.3.0-SNAPSHOT

import java.sql.Timestamp
type DeviceId = Int
case class Signal(timestamp: java.sql.Timestamp, value: Long, deviceId: DeviceId)

// input stream
import org.apache.spark.sql.functions._
val signals = spark.
  readStream.
  format("rate").
  option("rowsPerSecond", 1).
  load.
  withColumn("value", $"value" % 10).  // <-- randomize the values (just for fun)
  withColumn("deviceId", rint(rand() * 10) cast "int"). // <-- 10 devices randomly assigned to values
  as[Signal] // <-- convert to our type (from "unpleasant" Row)

scala> signals.explain
== Physical Plan ==
*Project [timestamp#0, (value#1L % 10) AS value#5L, cast(ROUND((rand(-664206172738708082) * 10.0)) as int) AS deviceId#9]
+- StreamingRelation rate, [timestamp#0, value#1L]

// stream processing using flatMapGroupsWithState operator
val device: Signal => DeviceId = { case Signal(_, _, deviceId) => deviceId }
val signalsByDevice = signals.groupByKey(device)

import org.apache.spark.sql.streaming.GroupState
type Key = Int
type Count = Long
type State = Map[Key, Count]
case class EventsCounted(deviceId: DeviceId, count: Long)
def countValuesPerKey(deviceId: Int, signalsPerDevice: Iterator[Signal], state: GroupState[State]): Iterator[EventsCounted] = {
  val values = signalsPerDevice.toList
  println(s"Device: $deviceId")
  println(s"Signals:")
  values.foreach { v => println(s"- $v") }
  println(s"State: $state")

  // update the state with the count of elements for the key
  val initialState: State = Map(deviceId -> 0)
  val oldState = state.getOption.getOrElse(initialState)
  // the name to highlight that the state is for the key only
  val newValue = oldState(deviceId) + values.size
  val newState = Map(deviceId -> newValue)
  state.update(newState)

  // you must not return as it's already consumed
  // that leads to a very subtle error where no elements are in an iterator
  // iterators are one-pass data structures
  Seq(EventsCounted(deviceId, newValue)).toIterator
}
import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode}
val signalCounter = signalsByDevice.flatMapGroupsWithState(
  outputMode = OutputMode.Append,
  timeoutConf = GroupStateTimeout.NoTimeout)(func = countValuesPerKey)

import org.apache.spark.sql.streaming.{OutputMode, Trigger}
import scala.concurrent.duration._
val sq = signalCounter.
  writeStream.
  format("console").
  option("truncate", false).
  trigger(Trigger.ProcessingTime(10.seconds)).
  outputMode(OutputMode.Append).
  start
...
-------------------------------------------
Batch: 0
-------------------------------------------
+--------+-----+
|deviceId|count|
+--------+-----+
+--------+-----+
...
17/08/20 20:31:58 INFO StreamExecution: Streaming query made progress: {
  "id" : "b9fe43e8-e173-4f3b-9bb6-91828bee0cb6",
  "runId" : "444f8c71-b769-4da4-9cb7-264f95d1b4a4",
  "name" : null,
  "timestamp" : "2017-08-20T18:31:55.858Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 2407,
    "getBatch" : 17,
    "getOffset" : 0,
    "queryPlanning" : 251,
    "triggerExecution" : 2721,
    "walCommit" : 32
  },
  "stateOperators" : [ {
    "numRowsTotal" : 0,
    "numRowsUpdated" : 0,
    "memoryUsedBytes" : 12599
  } ],
  "sources" : [ {
    "description" : "RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]",
    "startOffset" : null,
    "endOffset" : 0,
    "numInputRows" : 0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "ConsoleSink[numRows=20, truncate=false]"
  }
}
...
-------------------------------------------
Batch: 1
-------------------------------------------
Device: 5
Signals:
- Signal(2017-08-20 20:31:57.82,2,5)
Device: 3
Signals:
- Signal(2017-08-20 20:31:56.82,1,3)
State: GroupState(<undefined>)
State: GroupState(<undefined>)
Device: 8
Signals:
- Signal(2017-08-20 20:31:55.82,0,8)
- Signal(2017-08-20 20:31:58.82,3,8)
State: GroupState(<undefined>)
+--------+-----+
|deviceId|count|
+--------+-----+
|3       |1    |
|5       |1    |
|8       |2    |
+--------+-----+
...
17/08/20 20:32:01 INFO StreamExecution: Streaming query made progress: {
  "id" : "b9fe43e8-e173-4f3b-9bb6-91828bee0cb6",
  "runId" : "444f8c71-b769-4da4-9cb7-264f95d1b4a4",
  "name" : null,
  "timestamp" : "2017-08-20T18:32:00.000Z",
  "batchId" : 1,
  "numInputRows" : 4,
  "inputRowsPerSecond" : 0.9657170449058425,
  "processedRowsPerSecond" : 2.619515389652914,
  "durationMs" : {
    "addBatch" : 1439,
    "getBatch" : 24,
    "getOffset" : 0,
    "queryPlanning" : 23,
    "triggerExecution" : 1527,
    "walCommit" : 36
  },
  "stateOperators" : [ {
    "numRowsTotal" : 3,
    "numRowsUpdated" : 3,
    "memoryUsedBytes" : 18095
  } ],
  "sources" : [ {
    "description" : "RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]",
    "startOffset" : 0,
    "endOffset" : 4,
    "numInputRows" : 4,
    "inputRowsPerSecond" : 0.9657170449058425,
    "processedRowsPerSecond" : 2.619515389652914
  } ],
  "sink" : {
    "description" : "ConsoleSink[numRows=20, truncate=false]"
  }
}
...
-------------------------------------------
Batch: 2
-------------------------------------------
Device: 1
Signals:
- Signal(2017-08-20 20:32:03.82,8,1)
State: GroupState(<undefined>)
Device: 6
Signals:
- Signal(2017-08-20 20:32:08.82,3,6)
State: GroupState(<undefined>)
Device: 3
Signals:
- Signal(2017-08-20 20:32:02.82,7,3)
State: GroupState(Map(3 -> 1))
Device: 5
Signals:
- Signal(2017-08-20 20:32:05.82,0,5)
State: GroupState(Map(5 -> 1))
Device: 9
Signals:
- Signal(2017-08-20 20:32:04.82,9,9)
State: GroupState(<undefined>)
Device: 8
Signals:
- Signal(2017-08-20 20:32:00.82,5,8)
- Signal(2017-08-20 20:32:01.82,6,8)
- Signal(2017-08-20 20:32:07.82,2,8)
State: GroupState(Map(8 -> 2))
Device: 10
Signals:
- Signal(2017-08-20 20:32:06.82,1,10)
State: GroupState(<undefined>)
Device: 2
Signals:
- Signal(2017-08-20 20:31:59.82,4,2)
State: GroupState(<undefined>)
+--------+-----+
|deviceId|count|
+--------+-----+
|1       |1    |
|6       |1    |
|3       |2    |
|5       |2    |
|9       |1    |
|8       |5    |
|10      |1    |
|2       |1    |
+--------+-----+
...
17/08/20 20:32:10 INFO StreamExecution: Streaming query made progress: {
  "id" : "b9fe43e8-e173-4f3b-9bb6-91828bee0cb6",
  "runId" : "444f8c71-b769-4da4-9cb7-264f95d1b4a4",
  "name" : null,
  "timestamp" : "2017-08-20T18:32:10.006Z",
  "batchId" : 2,
  "numInputRows" : 10,
  "inputRowsPerSecond" : 0.9994003597841296,
  "processedRowsPerSecond" : 11.26126126126126,
  "durationMs" : {
    "addBatch" : 840,
    "getBatch" : 8,
    "getOffset" : 0,
    "queryPlanning" : 16,
    "triggerExecution" : 888,
    "walCommit" : 22
  },
  "stateOperators" : [ {
    "numRowsTotal" : 8,
    "numRowsUpdated" : 8,
    "memoryUsedBytes" : 19255
  } ],
  "sources" : [ {
    "description" : "RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]",
    "startOffset" : 4,
    "endOffset" : 14,
    "numInputRows" : 10,
    "inputRowsPerSecond" : 0.9994003597841296,
    "processedRowsPerSecond" : 11.26126126126126
  } ],
  "sink" : {
    "description" : "ConsoleSink[numRows=20, truncate=false]"
  }
}

// In the end...
sq.stop

// Use stateOperators to access the stats
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 7,
  "numRowsUpdated" : 7,
  "memoryUsedBytes" : 19023
}

scala> sq.explain
== Physical Plan ==
*SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, assertnotnull(input[0, $line17.$read$$iw$$iw$Signal, true]).ts, true, false) AS ts#30, assertnotnull(input[0, $line17.$read$$iw$$iw$Signal, true]).value AS value#31L, assertnotnull(input[0, $line17.$read$$iw$$iw$Signal, true]).deviceId AS deviceId#32]
+- FlatMapGroupsWithState <function3>, value#24: int, newInstance(class $line17.$read$$iw$$iw$Signal), [value#24], [ts#14, value#5L, deviceId#9], obj#29: $line17.$read$$iw$$iw$Signal, StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-3573bdcc-2046-4b9d-8377-53eeffd1bc2e/state,4e6a392e-4c0a-48a3-99c5-e39b6522811e,0,4), class[value[0]: int], Append, NoTimeout, 1503236740004, 0
   +- *Sort [value#24 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(value#24, 200)
         +- AppendColumns <function1>, newInstance(class $line17.$read$$iw$$iw$Signal), [input[0, int, false] AS value#24]
            +- *Project [timestamp#236 AS ts#14, (value#237L % 10) AS value#5L, 0 AS deviceId#9]
               +- Scan ExistingRDD[timestamp#236,value#237L]
----

Internally, `flatMapGroupsWithState` operator creates a `Dataset` with link:spark-sql-streaming-FlatMapGroupsWithState.adoc#apply[FlatMapGroupsWithState] unary logical operator.

CAUTION: FIXME Show the logical plan

`flatMapGroupsWithState` reports a `IllegalArgumentException` when the input `outputMode` is neither `Append` nor `Update`.

```
scala> val result = signalsByDevice.flatMapGroupsWithState(
     |   outputMode = OutputMode.Complete,
     |   timeoutConf = GroupStateTimeout.NoTimeout)(func = stateFn)
java.lang.IllegalArgumentException: The output mode of function should be append or update
  at org.apache.spark.sql.KeyValueGroupedDataset.flatMapGroupsWithState(KeyValueGroupedDataset.scala:381)
  ... 54 elided
```

CAUTION: FIXME Examples for append and update output modes (to demo the difference)

CAUTION: FIXME Examples for `GroupStateTimeout.EventTimeTimeout` with `withWatermark` operator
