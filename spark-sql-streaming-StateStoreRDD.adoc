== [[StateStoreRDD]] StateStoreRDD

`StateStoreRDD` is a specialized `RDD` for <<compute, computations>> using link:spark-sql-streaming-StateStore.adoc[StateStore].

`StateStoreRDD` is <<creating-instance, created>> as the result of link:spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore[executing physical operators] (i.e. `FlatMapGroupsWithStateExec`, `StateStoreRestoreExec`, `StateStoreSaveExec`, `StreamingDeduplicateExec`).

`StateStoreRDD` uses `StateStoreCoordinator` for <<getPreferredLocations, preferred locations>> for job scheduling.

.StateStoreRDD and StateStoreCoordinator
image::images/StateStoreRDD-StateStoreCoordinator.png[align="center"]

[[getPartitions]]
`getPartitions` is exactly the partitions of the <<dataRDD, data RDD>>.

[[internal-registries]]
.StateStoreRDD's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[hadoopConfBroadcast]] `hadoopConfBroadcast`
|

| [[storeConf]] `storeConf`
| Configuration parameters (as `StateStoreConf`) using the current `SQLConf` (from `SessionState`)
|===

=== [[compute]] Computing Partition (in TaskContext) -- `compute` Method

[source, scala]
----
compute(partition: Partition, ctxt: TaskContext): Iterator[U]
----

NOTE: `compute` is a part of the RDD Contract to compute a given partition in a `TaskContext`.

`compute` computes <<dataRDD, dataRDD>> and passing the result on to <<storeUpdateFunction, storeUpdateFunction>> (with a configured link:spark-sql-streaming-StateStore.adoc[StateStore]).

Internally, (and similarly to <<getPreferredLocations, getPreferredLocations>>) `compute` creates a `StateStoreProviderId` with `StateStoreId` (using <<checkpointLocation, checkpointLocation>>, <<operatorId, operatorId>> and the index of the input `partition`) and <<queryRunId, queryRunId>>.

`compute` then requests `StateStore` for link:spark-sql-streaming-StateStore.adoc#get[the store for the StateStoreProviderId].

In the end, `compute` computes <<dataRDD, dataRDD>> (using the input `partition` and `ctxt`) followed by executing <<storeUpdateFunction, storeUpdateFunction>> (with the store and the result).

=== [[getPreferredLocations]] Getting Placement Preferences of Partition -- `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(partition: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is a part of the RDD Contract to specify placement preferences (aka _preferred task locations_), i.e. where tasks should be executed to be as close to the data as possible.

`getPreferredLocations` creates a `StateStoreProviderId` with `StateStoreId` (using <<checkpointLocation, checkpointLocation>>, <<operatorId, operatorId>> and the index of the input `partition`) and <<queryRunId, queryRunId>>.

NOTE: <<checkpointLocation, checkpointLocation>> and <<operatorId, operatorId>> are shared across different partitions and so the only difference in `StateStoreProviderIds` is the partition index.

In the end, `getPreferredLocations` requests <<storeCoordinator, StateStoreCoordinatorRef>> for the location of the state store for `StateStoreProviderId`.

NOTE: link:spark-sql-streaming-StateStoreCoordinator.adoc[StateStoreCoordinator] coordinates instances of `StateStore` across all the executors, and tracks their locations for job scheduling.

=== [[creating-instance]] Creating StateStoreRDD Instance

`StateStoreRDD` takes the following when created:

* [[dataRDD]] The data `RDD` (of records of type `T`)
* [[storeUpdateFunction]] `storeUpdateFunction` (i.e. `(StateStore, Iterator[T]) => Iterator[U]`)
* [[checkpointLocation]] The path to the checkpoint location
* [[queryRunId]] `queryRunId`
* [[operatorId]] Operator id
* [[storeVersion]] Store version
* [[keySchema]] Schema of the keys
* [[valueSchema]] Schema of the values
* [[indexOrdinal]] Optional index
* [[sessionState]] `SessionState`
* [[storeCoordinator]] Optional `StateStoreCoordinatorRef`

`StateStoreRDD` initializes the <<internal-registries, internal registries and counters>>.
