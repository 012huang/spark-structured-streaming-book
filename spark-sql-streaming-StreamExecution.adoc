== [[StreamExecution]] StreamExecution -- Execution Environment for Streaming Datasets

`StreamExecution` represents the execution environment for continuous execution of streaming datasets (i.e. continuous execution of their streaming queries) every <<trigger, trigger>> and writing the result to a <<sink, streaming sink>>.

NOTE: The *streaming query* and *streaming Dataset* are synonyms and `StreamExecution` uses <<logicalPlan, analyzed logical plan>> to represent them.

`StreamExecution` is <<creating-instance, created>> exclusively when `DataStreamWriter` is link:spark-sql-streaming-DataStreamWriter.adoc#start[started].

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
val query = spark.
  readStream.
  format("rate").
  load.
  writeStream.
  format("console").
  trigger(Trigger.ProcessingTime("10 seconds")).
  start
----

.Creating Instance of StreamExecution
image::images/StreamExecution-creating-instance.png[align="center"]

NOTE: link:spark-sql-streaming-DataStreamWriter.adoc[DataStreamWriter] describes how the result of executing a streaming query is written to a streaming sink.

`StreamExecution` <<start, starts a thread of execution>> that runs a streaming query continuously and concurrently (and <<runBatches, polls for new records in the sources to create a batch>>).

.StreamExecution's Starting Streaming Query (on Execution Thread)
image::images/StreamExecution-start.png[align="center"]

`StreamExecution` can be in three states:

* `INITIALIZED` when the instance was created.
* `ACTIVE` when batches are pulled from the sources.
* `TERMINATED` when batches were successfully processed or the query stopped.

`StreamExecution` is also a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery].

[[internal-registries]]
.StreamExecution's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[availableOffsets]] `availableOffsets`
| link:spark-sql-streaming-StreamProgress.adoc[StreamProgress] with the streaming sources for which there are offsets available (i.e. unprocessed yet).

Used in <<constructNextBatch, constructNextBatch>> (to add the sources with offsets available) and...FIXME

| [[awaitBatchLock]] `awaitBatchLock`
| Java's fair reentrant mutual exclusion https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/ReentrantLock.html[java.util.concurrent.locks.ReentrantLock] (that favors granting access to the longest-waiting thread under contention).

| [[currentBatchId]] `currentBatchId`
a| Current batch number

* `-1` when `StreamExecution` is <<creating-instance, created>>

* `0` when `StreamExecution` <<populateStartOffsets, populateStartOffsets>> (when <<offsetLog, offsetLog>> is empty)

* Incremented when `StreamExecution` <<runBatches, runBatches>>

| [[currentDurationsMs]] `currentDurationsMs`
|

| [[id]] `id`
|

| [[initializationLatch]] `initializationLatch`
|

| [[logicalPlan]] `logicalPlan`
|

| [[microBatchThread]] `microBatchThread`
a| Thread of execution to run a streaming query concurrently with the name as `stream execution thread for [prettyIdString]` (that uses <<prettyIdString, prettyIdString>> for logging purposes).

When started, `microBatchThread` sets the so-called call site and <<runBatches, runs batches>>.

NOTE: `microBatchThread` is Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.util.Thread].

[TIP]
====
Use Java's http://docs.oracle.com/javase/8/docs/technotes/guides/management/jconsole.html[jconsole] or https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html[jstack] to monitor the streaming threads.

[options="wrap"]
----
$ jstack 13056 \| grep -e "stream execution thread"
"stream execution thread for kafka-topic1 [id = 609c5ea3-3e0b-4da9-9814-d0ad336dcadd, runId = 0717993d-e3f4-4e4b-81f5-f4c8a67e44b7]" #175 daemon prio=5 os_prio=31 tid=0x00007fe784978000 nid=0xc723 waiting on condition [0x0000000127cf0000]
----

====

| [[noNewData]] `noNewData`
| Flag whether there are any new offsets available for processing or not.

Turned on (i.e. enabled) in <<constructNextBatch, constructNextBatch>> when no new offsets are available.

| [[offsetLog]] `offsetLog`
|

| [[offsetSeqMetadata]] `offsetSeqMetadata`
|

| [[prettyIdString]] `prettyIdString`
a| Pretty-identified string for identification in logs (with <<name, name>> if defined).

```
// query name set
queryName [id = xyz, runId = abc]

// no query name
[id = xyz, runId = abc]
```

| [[resolvedCheckpointRoot]] `resolvedCheckpointRoot`
a| Qualified path of checkpoint directory (as defined using <<checkpointRoot, checkpointRoot>>).

[NOTE]
====
<<checkpointRoot, checkpointRoot>> is defined using `checkpointLocation` option or link:spark-sql-streaming-properties.adoc#spark-sql-streaming-properties.adoc[spark.sql.streaming.checkpointLocation] property with `queryName` option.

`checkpointLocation` and `queryName` options are passed in when `StreamingQueryManager` link:spark-sql-streaming-StreamingQueryManager.adoc#createQuery[creates the streaming query].
====

Used in <<checkpointFile, checkpointFile>> and when <<runBatches, runBatches>> finishes.

Used in <<logicalPlan, logicalPlan>> (where every link:spark-sql-streaming-StreamingRelation.adoc[StreamingRelation] stores its own checkpointing metadata).

[NOTE]
====
You can see `resolvedCheckpointRoot` in the INFO message when `StreamExecution` is <<start, started>>.

```
INFO Starting [id]. Use [resolvedCheckpointRoot] to store the query checkpoint.
```
====

Internally, `resolvedCheckpointRoot` creates a Hadoop `org.apache.hadoop.fs.Path` for <<checkpointRoot, checkpointRoot>> and makes it qualified.

NOTE: `resolvedCheckpointRoot` uses `SparkSession` to access `SessionState` for a Hadoop configuration.

| [[runId]] `runId`
|

| [[startLatch]] `startLatch`
| Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountDownLatch.html[java.util.concurrent.CountDownLatch] with count `1`.

Used when `StreamExecution` is <<start, started>> to get notified when `StreamExecution` <<runBatches, has posted a QueryStartedEvent>>.

| [[state]] `state`
a| Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReference.html[java.util.concurrent.atomic.AtomicReference] for the three different states a streaming query execution can be:

* `INITIALIZING` (default)
* `ACTIVE` (after the first <<runBatches, runBatches>>)
* `TERMINATED`

| [[streamMetadata]] `streamMetadata`
|

| [[uniqueSources]] `uniqueSources`
| Collection of unique link:spark-sql-streaming-Source.adoc[streaming sources]

Used in <<constructNextBatch, constructNextBatch>> (to get offsets for every source) and...FIXME
|===

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.sql.execution.streaming.StreamExecution` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[constructNextBatch]] Constructing Next Batch -- `constructNextBatch` Internal Method

[source, scala]
----
constructNextBatch(): Unit
----

`constructNextBatch` is made up of the two parts for when there is any data to process (and so where the next batch is constructed) and no data is available.

==== Checking Whether New Data Is Available (by Requesting New Offsets from Sources)

`constructNextBatch` firstly checks whether new data is available. It first acquires <<awaitBatchLock, awaitBatchLock>> and gets the offsets for <<uniqueSources, every streaming source used>>.

`constructNextBatch` <<updateStatusMessage, updates status message>> to the following for every source.

```
Getting offsets from [source]
```

`constructNextBatch` <<reportTimeTaken, reports the time>> for link:spark-sql-streaming-Source.adoc#getOffset[getting the offset per source].

`constructNextBatch` prints out the following DEBUG message in the logs:

```
DEBUG StreamExecution: getOffset took [time] ms
```

`constructNextBatch` adds the sources and their unprocessed offsets to <<availableOffsets, availableOffsets>>.

If there is no <<dataAvailable, data available>> (i.e. no offsets unprocessed in any of the sources), `constructNextBatch` turns <<noNewData, noNewData>> flag on.

In the end (of this block), `constructNextBatch` releases <<awaitBatchLock, awaitBatchLock>>

==== New Data Available

CAUTION: FIXME

==== No New Data Available

CAUTION: FIXME

NOTE: `constructNextBatch` is used when `StreamExecution` <<runBatches, runBatches>> and <<populateStartOffsets, populateStartOffsets>>.

=== [[runBatch]] `runBatch` Internal Method

[source, scala]
----
runBatch(sparkSessionToRunBatch: SparkSession): Unit
----

CAUTION: FIXME

NOTE: `runBatch` is used exclusively when `StreamExecution` <<runBatches, runs batches>>.

=== [[runBatches]] `runBatches` Internal Method

[source, scala]
----
runBatches(): Unit
----

`runBatches` runs streaming batches of data (that are datasets from every streaming source used).

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger.ProcessingTime
import scala.concurrent.duration._

val out = spark.
  readStream.
  text("server-logs").
  writeStream.
  format("console").
  queryName("debug").
  trigger(ProcessingTime(10.seconds))
scala> val debugStream = out.start
INFO StreamExecution: Starting debug [id = 8b57b0bd-fc4a-42eb-81a3-777d7ba5e370, runId = 920b227e-6d02-4a03-a271-c62120258cea]. Use file:///private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-274f9ae1-1238-4088-b4a1-5128fc520c1f to store the query checkpoint.
debugStream: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@58a5b69c

// Enable the log level to see the INFO and DEBUG messages
// log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG

17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query.
17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms
17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {}
17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms
17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: {
  "id" : "8b57b0bd-fc4a-42eb-81a3-777d7ba5e370",
  "runId" : "920b227e-6d02-4a03-a271-c62120258cea",
  "name" : "debug",
  "timestamp" : "2017-06-18T19:21:07.693Z",
  "numInputRows" : 0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 9
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]",
    "startOffset" : null,
    "endOffset" : null,
    "numInputRows" : 0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a"
  }
}
17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation
17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
----

Internally, `runBatches` sets the job group as <<runId, runId>>, <<getBatchDescriptionString, getBatchDescriptionString>> and `interruptOnCancel` flag enabled.

NOTE: `runBatches` uses <<sparkSession, SparkSession>> to access `SparkContext` and set the job group.

`runBatches` registers a metric source when link:spark-sql-streaming-properties.adoc#spark.sql.streaming.metricsEnabled[spark.sql.streaming.metricsEnabled] property is enabled (which is disabled by default).

CAUTION: FIXME Metrics

`runBatches` notifies `StreamingQueryListeners` that a streaming query has been started (by <<postEvent, posting a QueryStartedEvent>> with <<id, id>>, <<runId, runId>> and <<name, name>>).

`runBatches` unblocks the <<start, main starting thread>> (by decrementing the count of <<startLatch, startLatch>> that goes to `0` and lets the starting thread continue).

CAUTION: FIXME A picture with two parallel lanes for the starting thread and daemon one for the query.

`runBatches` <<updateStatusMessage, updates status message>> to *Initializing sources*.

`runBatches` then materializes the lazy <<logicalPlan, logicalPlan>>.

`runBatches` disables adaptive query execution (using `spark.sql.adaptive.enabled` property which is disabled by default) as it could change the number of shuffle partitions.

`runBatches` sets <<offsetSeqMetadata, offsetSeqMetadata>> variable.

`runBatches` sets <<state, state>> to `ACTIVE` (only when the current state is `INITIALIZING` that prevents from repeating the initialization)

NOTE: `runBatches` does the work only when first started (i.e. when <<state, state>> is `INITIALIZING`).

`runBatches` decrements the count of <<initializationLatch, initializationLatch>>.

CAUTION: FIXME `initializationLatch` so what?

`runBatches` requests <<triggerExecutor, TriggerExecutor>> to execute a so-called <<batch-runner, batch runner>>.

NOTE: `runBatches` is used exclusively when `StreamExecution` starts the <<microBatchThread, execution thread for a streaming query>> (i.e. the thread that runs the micro-batches of this stream).

==== [[batch-runner]] Batch Runner

*Batch Runner* (aka `batchRunner`) is an executable block executed by <<triggerExecutor, triggerExecutor>>.

`batchRunner` <<startTrigger, starts trigger calculation>>.

If <<state, state>> is not `TERMINATED` (which means that the streaming query should be executed), `batchRunner`  executes the current batch and <<reportTimeTaken, reports time taken>>.

The current batch branches off per <<currentBatchId, currentBatchId>>.

.Current Batch Execution per currentBatchId
[cols="1,1",options="header",width="100%"]
|===
| currentBatchId < 0
| currentBatchId >= 0

a|

1. <<populateStartOffsets, populateStartOffsets>>
1. Setting Job Description as <<getBatchDescriptionString, getBatchDescriptionString>>

```
DEBUG Stream running from [committedOffsets] to [availableOffsets]
```

| 1. <<constructNextBatch, constructNextBatch>>
|===

If there is <<dataAvailable, data available>>, `batchRunner` marks <<currentStatus, currentStatus>> with `isDataAvailable` enabled.

[NOTE]
====
You can check out the status of a link:spark-sql-streaming-StreamingQuery.adoc[streaming query] using link:spark-sql-streaming-StreamingQuery.adoc#status[status] method.

[source, scala]
----
scala> spark.streams.active(0).status
res1: org.apache.spark.sql.streaming.StreamingQueryStatus =
{
  "message" : "Waiting for next trigger",
  "isDataAvailable" : false,
  "isTriggerActive" : false
}
----
====

`batchRunner` then <<updateStatusMessage, updates the status message>> to "Processing new data" and <<runBatch, runs the batch>>.

.StreamExecution's Running Batches (on Execution Thread)
image::images/StreamExecution-runBatches.png[align="center"]

CAUTION: FIXME Finish me...`finishTrigger(dataAvailable)`

=== [[dataAvailable]] `dataAvailable` Internal Method

CAUTION: FIXME

=== [[populateStartOffsets]] `populateStartOffsets` Internal Method

[source, scala]
----
populateStartOffsets(sparkSessionToRunBatches: SparkSession): Unit
----

CAUTION: FIXME

NOTE: `populateStartOffsets` is used exclusively when <<triggerExecutor, triggerExecutor>> executes a trigger for the first time (when <<currentBatchId, currentBatchId>> is negative).

=== [[reportTimeTaken]] `reportTimeTaken` Internal Method

CAUTION: FIXME

=== [[updateStatusMessage]] `updateStatusMessage` Internal Method

CAUTION: FIXME

=== [[postEvent]] `postEvent` Internal Method

CAUTION: FIXME

=== [[getBatchDescriptionString]] `getBatchDescriptionString` Internal Method

[source, scala]
----
getBatchDescriptionString: String
----

CAUTION: FIXME

=== [[toDebugString]] `toDebugString` Method

You can call `toDebugString` on `StreamExecution` to learn about the internals.

```
scala> out.asInstanceOf[StreamExecution].toDebugString
res3: String =
"
=== Continuous Query ===
Name: memStream
Current Offsets: {FileSource[hello]: #0}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
FileSource[hello]


     "
```

NOTE: `toDebugString` is used exclusively when `StreamExecution` <<runBatches, runBatches>> (when a streaming query terminated with exception).

=== [[start]] Starting Streaming Query (on Execution Thread) -- `start` Method

[source, scala]
----
start(): Unit
----

When called, `start` prints the following INFO message to the logs:

```
INFO Starting [id]. Use [resolvedCheckpointRoot] to store the query checkpoint.
```

`start` then sets <<microBatchThread, microBatchThread>> as a daemon thread and starts it.

NOTE: `start` uses Java's link:++https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start] to run the streaming query on a separate execution thread.

NOTE: When started, a streaming query runs in its own execution thread on JVM.

In the end, `start` waits until <<startLatch, startLatch>> has counted down to zero (which is when `StreamExecution` <<runBatches, starts running batches>> so there is some pause in the main thread's execution to wait till the streaming query execution thread starts).

NOTE: `start` is used exclusively when `StreamingQueryManager` is requested to link:spark-sql-streaming-StreamingQueryManager.adoc#startQuery[start a streaming query].

=== [[creating-instance]] Creating StreamExecution Instance

`StreamExecution` takes the following when created:

* [[sparkSession]] `SparkSession`
* [[name]] Query name
* [[checkpointRoot]] Path to the checkpoint root directory
* [[analyzedPlan]] Analyzed logical plan
* [[sink]] link:spark-sql-streaming-Sink.adoc[Streaming sink]
* [[trigger]] link:spark-sql-streaming-Trigger.adoc[Trigger]
* [[triggerClock]] `Clock`
* [[outputMode]] link:spark-sql-streaming-OutputMode.adoc[Output mode]
* [[deleteCheckpointOnStop]] Flag where to delete the checkpoint on stop

`StreamExecution` initializes the <<internal-registries, internal registries and counters>>.

=== [[checkpointFile]] `checkpointFile` Internal Method

[source, scala]
----
checkpointFile(name: String): String
----

`checkpointFile` gives the path of a file with `name` in <<resolvedCheckpointRoot, checkpoint directory>>.

NOTE: `checkpointFile` uses Hadoop's `org.apache.hadoop.fs.Path`.

NOTE: `checkpointFile` is used for <<streamMetadata, streamMetadata>>, <<offsetLog, offsetLog>>, <<batchCommitLog, batchCommitLog>>, and <<lastExecution, lastExecution>> (for <<runBatch, runBatch>>).
