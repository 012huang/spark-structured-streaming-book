== [[StreamExecution]] StreamExecution

`StreamExecution` manages execution of a streaming query for a `SQLContext` and a link:spark-sql-streaming-sink.adoc[Sink]. It requires a link:spark-sql-LogicalPlan.adoc[LogicalPlan] to know the `Source` objects from which records are periodically pulled down.

`StreamExecution` is a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] with additional attributes:

* [[checkpointRoot]] `checkpointRoot`
* link:spark-sql-LogicalPlan.adoc[LogicalPlan]
* link:spark-sql-streaming-sink.adoc[Sink]
* `Trigger`

It starts an internal thread (`microBatchThread`) to periodically (every 10 milliseconds) poll for new records in the sources and create a batch.

NOTE: The time between batches - 10 milliseconds - is fixed (i.e. not configurable).

`StreamExecution` can be in three states:

* `INITIALIZED` when the instance was created.
* `ACTIVE` when batches are pulled from the sources.
* `TERMINATED` when batches were successfully processed or the query stopped.

[[internal-registries]]
.StreamExecution's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[microBatchThread]] `microBatchThread`
|

| [[startLatch]] `startLatch`
| Java's `java.util.concurrent.CountDownLatch` with count `1`.

Used when `StreamExecution` is <<start, started>> to get notified when `StreamExecution` <<runBatches, posts a QueryStartedEvent>>.
|===

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.sql.execution.streaming.StreamExecution` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[start]] `start` Method

[source, scala]
----
start(): Unit
----

Internally, when called, `start` prints the following INFO message to the logs:

```
INFO Starting [id]. Use [resolvedCheckpointRoot] to store the query checkpoint.
```

`start` then sets <<microBatchThread, microBatchThread>> as a daemon thread and starts it.

NOTE: `start` uses Java's `java.lang.Thread.start` to start the query.

NOTE: When started, streaming query runs in its own thread on JVM.

In the end, `start` waits until <<startLatch, startLatch>> has counted down to zero which is...

NOTE: `start` is used exclusively when `StreamingQueryManager` is requested to link:spark-sql-streaming-StreamingQueryManager.adoc#startQuery[start a streaming query].

=== [[runBatches]] `runBatches` Internal Method

[source, scala]
----
runBatches(): Unit
----

CAUTION: FIXME

```
scala> val out = in.write
  .format("memory")
  .queryName("memStream")
  .startStream()
out: org.apache.spark.sql.StreamingQuery = Continuous Query - memStream [state = ACTIVE]
16/04/16 00:48:47 INFO StreamExecution: Starting new continuous query.

scala> 16/04/16 00:48:47 INFO StreamExecution: Committed offsets for batch 1.
16/04/16 00:48:47 DEBUG StreamExecution: Stream running from {} to {FileSource[hello]: #0}
16/04/16 00:48:47 DEBUG StreamExecution: Retrieving data from FileSource[hello]: None -> #0
16/04/16 00:48:47 DEBUG StreamExecution: Optimized batch in 163.940239ms
16/04/16 00:48:47 INFO StreamExecution: Completed up to {FileSource[hello]: #0} in 703.573981ms
```

=== [[toDebugString]] `toDebugString` Method

You can call `toDebugString` on `StreamExecution` to learn about the internals.

```
scala> out.asInstanceOf[StreamExecution].toDebugString
res3: String =
"
=== Continuous Query ===
Name: memStream
Current Offsets: {FileSource[hello]: #0}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
FileSource[hello]


     "
```
