== [[StreamExecution]] StreamExecution

`StreamExecution` manages execution of a streaming query for a `SQLContext` and a link:spark-sql-streaming-sink.adoc[Sink]. It requires a link:spark-sql-LogicalPlan.adoc[LogicalPlan] to know the `Source` objects from which records are periodically pulled down.

`StreamExecution` is a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] with additional attributes:

* [[checkpointRoot]] `checkpointRoot`
* link:spark-sql-LogicalPlan.adoc[LogicalPlan]
* link:spark-sql-streaming-sink.adoc[Sink]
* `Trigger`

It starts an internal thread (`microBatchThread`) to periodically (every 10 milliseconds) poll for new records in the sources and create a batch.

NOTE: The time between batches - 10 milliseconds - is fixed (i.e. not configurable).

`StreamExecution` can be in three states:

* `INITIALIZED` when the instance was created.
* `ACTIVE` when batches are pulled from the sources.
* `TERMINATED` when batches were successfully processed or the query stopped.

[[internal-registries]]
.StreamExecution's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[currentDurationsMs]] `currentDurationsMs`
|

| [[id]] `id`
|

| [[initializationLatch]] `initializationLatch`
|

| [[logicalPlan]] `logicalPlan`
|

| [[microBatchThread]] `microBatchThread`
a| Thread of execution to run a streaming query concurrently with the name as `stream execution thread for [prettyIdString]` (that uses <<prettyIdString, prettyIdString>> for logging purposes).

When started, `microBatchThread` sets the so-called call site and <<runBatches, runs batches>>.

NOTE: `microBatchThread` is Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.util.Thread].

[TIP]
====
Use Java's http://docs.oracle.com/javase/8/docs/technotes/guides/management/jconsole.html[jconsole] or https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html[jstack] to monitor the streaming threads.

```
$ jstack 13056 | grep -i "stream execution thread"
"stream execution thread for kafka-topic1 [id = 609c5ea3-3e0b-4da9-9814-d0ad336dcadd, runId = 0717993d-e3f4-4e4b-81f5-f4c8a67e44b7]" #175 daemon prio=5 os_prio=31 tid=0x00007fe784978000 nid=0xc723 waiting on condition [0x0000000127cf0000]
```
====

| [[offsetSeqMetadata]] `offsetSeqMetadata`
|

| [[prettyIdString]] `prettyIdString`
a| Pretty-identified string for identification in logs (with <<name, name>> if defined).

```
// query name set
queryName [id = xyz, runId = abc]

// no query name
[id = xyz, runId = abc]
```

| [[runId]] `runId`
|

| [[startLatch]] `startLatch`
| Java's `java.util.concurrent.CountDownLatch` with count `1`.

Used when `StreamExecution` is <<start, started>> to get notified when `StreamExecution` <<runBatches, posts a QueryStartedEvent>>.

| [[state]] `state`
a| Java's `java.util.concurrent.atomic.AtomicReference` for the three different states a streaming query execution can be:

* `INITIALIZING` (default)
* `ACTIVE` (after the first <<runBatches, runBatches>>)
* `TERMINATED`

| [[streamMetadata]] `streamMetadata`
|

| [[uniqueSources]] `uniqueSources`
|
|===

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.sql.execution.streaming.StreamExecution` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[runBatch]] `runBatch` Internal Method

[source, scala]
----
runBatch(sparkSessionToRunBatch: SparkSession): Unit
----

CAUTION: FIXME

NOTE: `runBatch` is used exclusively when `StreamExecution` <<runBatches, runs batches>>.

=== [[constructNextBatch]] `constructNextBatch` Internal Method

CAUTION: FIXME

=== [[runBatches]] `runBatches` Internal Method

[source, scala]
----
runBatches(): Unit
----

`runBatches` runs streaming batches of data (that are datasets from every streaming source used).

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger.ProcessingTime
import scala.concurrent.duration._

val out = spark.
  readStream.
  text("server-logs").
  writeStream.
  format("console").
  queryName("debug").
  trigger(ProcessingTime(10.seconds))
scala> val debugStream = out.start
INFO StreamExecution: Starting debug [id = 8b57b0bd-fc4a-42eb-81a3-777d7ba5e370, runId = 920b227e-6d02-4a03-a271-c62120258cea]. Use file:///private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-274f9ae1-1238-4088-b4a1-5128fc520c1f to store the query checkpoint.
debugStream: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@58a5b69c

// Enable the log level to see the INFO and DEBUG messages
// log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG

17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query.
17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms
17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {}
17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms
17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: {
  "id" : "8b57b0bd-fc4a-42eb-81a3-777d7ba5e370",
  "runId" : "920b227e-6d02-4a03-a271-c62120258cea",
  "name" : "debug",
  "timestamp" : "2017-06-18T19:21:07.693Z",
  "numInputRows" : 0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 9
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]",
    "startOffset" : null,
    "endOffset" : null,
    "numInputRows" : 0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a"
  }
}
17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation
17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
----

Internally, `runBatches` sets the job group as <<runId, runId>>, <<getBatchDescriptionString, getBatchDescriptionString>> and `interruptOnCancel` flag enabled.

NOTE: `runBatches` uses <<sparkSession, SparkSession>> to access `SparkContext` and set the job group.

`runBatches` registers a metric source when `spark.sql.streaming.metricsEnabled` property is enabled (which is disabled by default).

`runBatches` notifies `StreamingQueryListeners` that a streaming query has been started (by <<postListenerEvent, posting a QueryStartedEvent>> with <<id, id>>, <<runId, runId>> and <<name, name>>).

`runBatches` unblocks the <<start, starting thread>> (by decrementing the count of <<startLatch, startLatch>> that goes to `0` and lets the starting thread finish).

CAUTION: FIXME A picture with two parallel lanes for the starting thread and daemon one for the query.

`runBatches` <<updateStatusMessage, updates status message>> to *Initializing sources*.

`runBatches` then materializes the lazy <<logicalPlan, logicalPlan>>.

`runBatches` disables adaptive query execution (using `spark.sql.adaptive.enabled` property which is disabled by default) as it could change the number of shuffle partitions.

`runBatches` sets <<offsetSeqMetadata, offsetSeqMetadata>> variable.

In the end, `runBatches` sets <<state, state>> to `ACTIVE` state.

NOTE: `runBatches` does the work only when first started (i.e. when <<state, state>> is `INITIALIZING`).

CAUTION: FIXME Continue if you don't mind.

NOTE: `runBatches` is used exclusively when `StreamExecution` starts the <<microBatchThread, execution thread for a streaming query>> (i.e. the thread that runs the micro-batches of this stream).

=== [[updateStatusMessage]] `updateStatusMessage` Internal Method

CAUTION: FIXME

=== [[postListenerEvent]] `postListenerEvent` Internal Method

CAUTION: FIXME

=== [[getBatchDescriptionString]] `getBatchDescriptionString` Internal Method

[source, scala]
----
getBatchDescriptionString: String
----

CAUTION: FIXME

=== [[toDebugString]] `toDebugString` Method

You can call `toDebugString` on `StreamExecution` to learn about the internals.

```
scala> out.asInstanceOf[StreamExecution].toDebugString
res3: String =
"
=== Continuous Query ===
Name: memStream
Current Offsets: {FileSource[hello]: #0}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
FileSource[hello]


     "
```

=== [[start]] Starting Streaming Query (on Execution Thread) -- `start` Method

[source, scala]
----
start(): Unit
----

When called, `start` prints the following INFO message to the logs:

```
INFO Starting [id]. Use [resolvedCheckpointRoot] to store the query checkpoint.
```

.StreamExecution's Starting Streaming Query (on Execution Thread)
image::images/StreamExecution-start.png[align="center"]

`start` then sets <<microBatchThread, microBatchThread>> as a daemon thread and starts it.

NOTE: `start` uses Java's link:++https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start] to run the streaming query on a separate execution thread.

NOTE: When started, a streaming query runs in its own execution thread on JVM.

In the end, `start` waits until <<startLatch, startLatch>> has counted down to zero (which is when `StreamExecution` <<runBatches, starts running batches>> so there is some pause in the main thread's execution to wait till the streaming query execution thread starts).

NOTE: `start` is used exclusively when `StreamingQueryManager` is requested to link:spark-sql-streaming-StreamingQueryManager.adoc#startQuery[start a streaming query].
