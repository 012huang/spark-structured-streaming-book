== [[KafkaSourceProvider]] KafkaSourceProvider

`KafkaSourceProvider` is a link:spark-sql-streaming-StreamSourceProvider.adoc[StreamSourceProvider] for link:spark-sql-streaming-KafkaSource.adoc[KafkaSource] (that acts as the source for *kafka* format).

NOTE: `KafkaSourceProvider` is also a `DataSourceRegister`.

[[shortName]]
The short name of the data source is *kafka*.

`KafkaSourceProvider` requires the following options (that you can set using `option` method of `DataStreamReader` or `DataStreamWriter`):

1. Exactly one option for `subscribe`, `subscribepattern` or `assign`
2. `kafka.bootstrap.servers` (that becomes Kafka's `bootstrap.servers` property)

TIP: Refer to link:spark-sql-streaming-KafkaSource.adoc#options[KafkaSource's Options] for the supported options.

NOTE: `KafkaSourceProvider` is part of <<spark-sql-kafka-0-10, `spark-sql-kafka-0-10` Library Dependency>>.

[source, scala]
----
// Run spark-shell with spark-sql-kafka-0-10_2.11 module
val records = spark.
  readStream.
  format("kafka").  // <-- use KafkaSourceProvider
  option("subscribe", "topic").
  option("kafka.bootstrap.servers", "localhost:9092").
  option("startingoffsets", "earliest").
  option("maxOffsetsPerTrigger", 1).
  load.
  select(
    $"key" cast "string",   // deserialize keys
    $"value" cast "string", // deserialize values
    $"topic",
    $"partition",
    $"offset")
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
import scala.concurrent.duration._
val q = records.
  writeStream.
  format("console").
  option("truncate", false).
  trigger(Trigger.ProcessingTime(10.seconds)).
  outputMode(OutputMode.Append).
  queryName("from-kafka-to-console").
  start
----

=== [[createRelation]] `createRelation` Method

CAUTION: FIXME

=== [[validateStreamOptions]] `validateStreamOptions` Internal Method

CAUTION: FIXME

=== [[createSource]] Creating KafkaSource -- `createSource` Method

[source, scala]
----
createSource(
  sqlContext: SQLContext,
  metadataPath: String,
  schema: Option[StructType],
  providerName: String,
  parameters: Map[String, String]): Source
----

Internally, `createSource` first <<validateStreamOptions, validates stream options>>.

CAUTION: FIXME

NOTE: `createSource` is a part of link:spark-sql-streaming-StreamSourceProvider.adoc#createSource[StreamSourceProvider Contract] to create a streaming source for *kafka* format.

=== [[spark-sql-kafka-0-10]] spark-sql-kafka-0-10 Library Dependency

The new structured streaming API for Kafka is part of the `spark-sql-kafka-0-10` artifact. Add the following dependency to sbt project to use the streaming integration:

```
libraryDependencies += "org.apache.spark" %% "spark-sql-kafka-0-10" % "2.2.0"
```

[TIP]
====
`spark-sql-kafka-0-10` module is not included in the CLASSPATH of link:spark-shell.adoc[spark-shell] so you have to start it with link:spark-submit.adoc#packages[`--packages` command-line option].

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0
```
====

NOTE: Replace `2.2.0` or `2.3.0-SNAPSHOT` with one of the available versions found at http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22spark-streaming-kafka-0-10_2.11%22[The Central Repository's Search] that matches your version of Spark.

=== [[validateGeneralOptions]] Validating General Options For Kafka -- `validateGeneralOptions` Internal Method

[source, scala]
----
validateGeneralOptions(parameters: Map[String, String]): Unit
----

NOTE: Parameters are case-insensitive, i.e. `OptioN` and `option` are equal.

`validateGeneralOptions` makes sure that exactly one topic subscription strategy is used in `parameters` and can be:

* `subscribe`
* `subscribepattern`
* `assign`

`validateGeneralOptions` reports an `IllegalArgumentException` when there is no subscription strategy in use or there are more than one strategies used.

`validateGeneralOptions` makes sure that the value of subscription strategies meet the requirements:

* `assign` strategy starts with `{` (the opening curly brace)
* `subscribe` strategy has at least one topic (in a comma-separated list of topics)
* `subscribepattern` strategy has the pattern defined

`validateGeneralOptions` makes sure that `group.id` has not been specified and reports an `IllegalArgumentException` otherwise.

```
Kafka option 'group.id' is not supported as user-specified consumer groups are not used to track offsets.
```

`validateGeneralOptions` makes sure that `auto.offset.reset` has not been specified and reports an `IllegalArgumentException` otherwise.

[options="wrap"]
----
Kafka option 'auto.offset.reset' is not supported.
Instead set the source option 'startingoffsets' to 'earliest' or 'latest' to specify where to start. Structured Streaming manages which offsets are consumed internally, rather than relying on the kafkaConsumer to do it. This will ensure that no data is missed when new topics/partitions are dynamically subscribed. Note that 'startingoffsets' only applies when a new Streaming query is started, and
that resuming will always pick up from where the query left off. See the docs for more details.
----

`validateGeneralOptions` makes sure that the following options have not been specified and reports an `IllegalArgumentException` otherwise:

* `kafka.key.deserializer`
* `kafka.value.deserializer`
* `kafka.enable.auto.commit`
* `kafka.interceptor.classes`

In the end, `validateGeneralOptions` makes sure that `kafka.bootstrap.servers` option was specified and reports an `IllegalArgumentException` otherwise.

```
Option 'kafka.bootstrap.servers' must be specified for configuring Kafka consumer
```

NOTE: `validateGeneralOptions` is used when `KafkaSourceProvider` validates options for <<validateStreamOptions, streaming>> and <<validateBatchOptions, batch>> modes.

=== [[strategy]] Creating ConsumerStrategy -- `strategy` Internal Method

[source, scala]
----
strategy(caseInsensitiveParams: Map[String, String])
----

Internally, `strategy` finds the keys in the input `caseInsensitiveParams` that are one of the following and creates a corresponding link:spark-sql-streaming-ConsumerStrategy.adoc[ConsumerStrategy].

.KafkaSourceProvider.strategy's Key to ConsumerStrategy Conversion
[cols="1,2",options="header",width="100%"]
|===
| Key
| ConsumerStrategy

| `assign`
a| link:spark-sql-streaming-ConsumerStrategy.adoc#AssignStrategy[AssignStrategy] with Kafka's http://kafka.apache.org/0110/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartitions].

---

`strategy` uses `JsonUtils.partitions` method to parse a JSON with topic names and partitions, e.g.

```
{"topicA":[0,1],"topicB":[0,1]}
```

The topic names and partitions are mapped directly to Kafka's `TopicPartition` objects.

| `subscribe`
a| link:spark-sql-streaming-ConsumerStrategy.adoc#SubscribeStrategy[SubscribeStrategy] with topic names

---

`strategy` extracts topic names from a comma-separated string, e.g.

```
topic1,topic2,topic3
```

| `subscribepattern`
| link:spark-sql-streaming-ConsumerStrategy.adoc#SubscribePatternStrategy[SubscribePatternStrategy]
|===

[NOTE]
====
`strategy` is used when:

* `KafkaSourceProvider` <<createSource, creates a KafkaOffsetReader for KafkaSource>>.

* `KafkaSourceProvider` <<createRelation, creates a KafkaRelation>>.
====
