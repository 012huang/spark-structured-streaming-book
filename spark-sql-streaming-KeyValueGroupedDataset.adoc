== [[KeyValueGroupedDataset]] KeyValueGroupedDataset

`KeyValueGroupedDataset` is created by `Dataset.groupByKey` operator (that groups rows given condition defined by a grouping function).

[source, scala]
----
val nums = spark.range(5)
scala> val grouped = nums.groupByKey(n => n % 2)
grouped: org.apache.spark.sql.KeyValueGroupedDataset[Long,Long] = org.apache.spark.sql.KeyValueGroupedDataset@76c6ded8
----

`KeyValueGroupedDataset` shines the most when used with streaming Datasets as is then executed continuously (and you can experience how the grouping state works over time).

[source, scala]
----
import java.sql.Timestamp
val numGroups = spark.
  readStream.
  format("rate").
  load.
  as[(Timestamp, Long)].
  groupByKey { case (time, value) => value % 2 }
----

[[methods]]
.KeyValueGroupedDataset's Methods
[cols="1,2",options="header",width="100%"]
|===
| Method | Description

| <<agg, agg>>
|

| <<cogroup, cogroup>>
|

| <<count, count>>
|

| <<flatMapGroups, flatMapGroups>>
|

| <<flatMapGroupsWithState, flatMapGroupsWithState>>
|

| <<keyAs, keyAs>>
|

| <<keys, keys>>
|

| <<mapGroups, mapGroups>>
|

| <<mapGroupsWithState, mapGroupsWithState>>
|

| <<mapValues, mapValues>>
|

| <<queryExecution, queryExecution>>
|

| <<reduceGroups, reduceGroups>>
|
|===

=== [[flatMapGroupsWithState]] `flatMapGroupsWithState` Method

[source, scala]
----
flatMapGroupsWithState[S: Encoder, U: Encoder](
  outputMode: OutputMode,
  timeoutConf: GroupStateTimeout)(
  func: (K, Iterator[V], GroupState[S]) => Iterator[U]): Dataset[U]
----

NOTE: `flatMapGroupsWithState` requires `Append` or `Update` link:spark-sql-streaming-OutputMode.adoc[output modes].

`flatMapGroupsWithState` creates a `Dataset` with link:spark-sql-streaming-FlatMapGroupsWithState.adoc[FlatMapGroupsWithState] logical operator.

CAUTION: FIXME

=== [[mapGroupsWithState]] `mapGroupsWithState` Method

[source, scala]
----
mapGroupsWithState[S: Encoder, U: Encoder](
  func: (K, Iterator[V], GroupState[S]) => U): Dataset[U] // <1>
mapGroupsWithState[S: Encoder, U: Encoder](
  timeoutConf: GroupStateTimeout)(
  func: (K, Iterator[V], GroupState[S]) => U): Dataset[U]
----
<1> Uses `GroupStateTimeout.NoTimeout`

NOTE: `mapGroupsWithState` is <<flatMapGroupsWithState, flatMapGroupsWithState>> in which `func` is transformed to return a single-element `Iterator`.

[source, scala]
----
// numGroups defined at the beginning
scala> :type numGroups
org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]

import org.apache.spark.sql.streaming.GroupState
def mappingFunc(key: Long, values: Iterator[(java.sql.Timestamp, Long)], state: GroupState[Long]): Long = {
  println(s">>> key: $key => state: $state")
  val newState = state.getOption.map(_ + values.size).getOrElse(0L)
  state.update(newState)
  key
}

import org.apache.spark.sql.streaming.GroupStateTimeout
val longs = numGroups.mapGroupsWithState(
    timeoutConf = GroupStateTimeout.ProcessingTimeTimeout)(
    func = mappingFunc)

import org.apache.spark.sql.streaming.{OutputMode, Trigger}
import scala.concurrent.duration._
val q = longs.
  writeStream.
  format("console").
  trigger(Trigger.ProcessingTime(10.seconds)).
  outputMode(OutputMode.Update). // <-- required for mapGroupsWithState
  start

// Note GroupState

-------------------------------------------
Batch: 1
-------------------------------------------
>>> key: 0 => state: GroupState(<undefined>)
>>> key: 1 => state: GroupState(<undefined>)
+-----+
|value|
+-----+
|    0|
|    1|
+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
>>> key: 0 => state: GroupState(0)
>>> key: 1 => state: GroupState(0)
+-----+
|value|
+-----+
|    0|
|    1|
+-----+

-------------------------------------------
Batch: 3
-------------------------------------------
>>> key: 0 => state: GroupState(4)
>>> key: 1 => state: GroupState(4)
+-----+
|value|
+-----+
|    0|
|    1|
+-----+

// in the end
spark.streams.active(0).stop
----
