== [[KeyValueGroupedDataset]] KeyValueGroupedDataset -- (Stateful) Streaming Aggregation

`KeyValueGroupedDataset` represents a *grouped dataset* as a result of `Dataset.groupByKey` operator (that groups rows given condition defined by a grouping function).

NOTE: `KeyValueGroupedDataset` can also be used for batch aggregations.

[source, scala]
----
val nums = spark.range(5)
scala> val grouped = nums.groupByKey(n => n % 2)
grouped: org.apache.spark.sql.KeyValueGroupedDataset[Long,Long] = org.apache.spark.sql.KeyValueGroupedDataset@76c6ded8
----

`KeyValueGroupedDataset` shines the most when used for *streaming aggregation* (with streaming Datasets).

[source, scala]
----
import java.sql.Timestamp
scala> val numGroups = spark.
  readStream.
  format("rate").
  load.
  as[(Timestamp, Long)].
  groupByKey { case (time, value) => value % 2 }
numGroups: org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] = org.apache.spark.sql.KeyValueGroupedDataset@616c1605

import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
numGroups.
  mapGroups { case(group, values) => values.size }.
  writeStream.
  format("console").
  trigger(Trigger.ProcessingTime(10.seconds)).
  start

-------------------------------------------
Batch: 0
-------------------------------------------
+-----+
|value|
+-----+
+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+-----+
|value|
+-----+
|    3|
|    2|
+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----+
|value|
+-----+
|    5|
|    5|
+-----+

// Eventually...
spark.streams.active(0).stop
----

The most prestigious use case of `KeyValueGroupedDataset` however is *stateful streaming aggregation* that allows for accumulating *streaming state* (by means of link:spark-sql-streaming-GroupState.adoc[GroupState]) using <<mapGroupsWithState, mapGroupsWithState>> and the more advanced <<flatMapGroupsWithState, flatMapGroupsWithState>> operators.

[[methods]]
.KeyValueGroupedDataset's Methods
[cols="1,2",options="header",width="100%"]
|===
| Method | Description

| <<agg, agg>>
|

| <<cogroup, cogroup>>
|

| <<count, count>>
|

| <<flatMapGroups, flatMapGroups>>
|

| <<flatMapGroupsWithState, flatMapGroupsWithState>>
a| Creates a `Dataset` with link:spark-sql-streaming-FlatMapGroupsWithState.adoc#apply[FlatMapGroupsWithState] logical operator

NOTE: The difference between `flatMapGroupsWithState` and <<mapGroupsWithState, mapGroupsWithState>> is the state function that generates zero or more elements (that are in turn the rows in the result `Dataset`).

| <<keyAs, keyAs>>
|

| <<keys, keys>>
|

| <<mapGroups, mapGroups>>
|

| <<mapGroupsWithState, mapGroupsWithState>>
a| Creates a `Dataset` with link:spark-sql-streaming-FlatMapGroupsWithState.adoc#apply[FlatMapGroupsWithState] logical operator

NOTE: The difference between `mapGroupsWithState` and <<flatMapGroupsWithState, flatMapGroupsWithState>> is the state function that generates exactly one element (that is in turn the row in the result `Dataset`).

| <<mapValues, mapValues>>
|

| <<queryExecution, queryExecution>>
|

| <<reduceGroups, reduceGroups>>
|
|===

=== [[flatMapGroupsWithState]] Creating Dataset with FlatMapGroupsWithState Logical Operator -- `flatMapGroupsWithState` Method

[source, scala]
----
flatMapGroupsWithState[S: Encoder, U: Encoder](
  outputMode: OutputMode,
  timeoutConf: GroupStateTimeout)(
  func: (K, Iterator[V], GroupState[S]) => Iterator[U]): Dataset[U]
----

NOTE: `flatMapGroupsWithState` requires `Append` or `Update` link:spark-sql-streaming-OutputMode.adoc[output modes].

`flatMapGroupsWithState` creates a `Dataset` with link:spark-sql-streaming-FlatMapGroupsWithState.adoc#apply[FlatMapGroupsWithState] logical operator.

`flatMapGroupsWithState` reports a `IllegalArgumentException` when the input `outputMode` is not `Append` nor `Update`.

```
The output mode of function should be append or update
```

=== [[mapGroupsWithState]] Creating Dataset with FlatMapGroupsWithState Logical Operator -- `mapGroupsWithState` Method

[source, scala]
----
mapGroupsWithState[S: Encoder, U: Encoder](
  func: (K, Iterator[V], GroupState[S]) => U): Dataset[U] // <1>
mapGroupsWithState[S: Encoder, U: Encoder](
  timeoutConf: GroupStateTimeout)(
  func: (K, Iterator[V], GroupState[S]) => U): Dataset[U]
----
<1> Uses `GroupStateTimeout.NoTimeout`

NOTE: `mapGroupsWithState` is <<flatMapGroupsWithState, flatMapGroupsWithState>> in which `func` is transformed to return a single-element `Iterator`.

[source, scala]
----
// numGroups defined at the beginning
scala> :type numGroups
org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]

import org.apache.spark.sql.streaming.GroupState
def mappingFunc(key: Long, values: Iterator[(java.sql.Timestamp, Long)], state: GroupState[Long]): Long = {
  println(s">>> key: $key => state: $state")
  val newState = state.getOption.map(_ + values.size).getOrElse(0L)
  state.update(newState)
  key
}

import org.apache.spark.sql.streaming.GroupStateTimeout
val longs = numGroups.mapGroupsWithState(
    timeoutConf = GroupStateTimeout.ProcessingTimeTimeout)(
    func = mappingFunc)

import org.apache.spark.sql.streaming.{OutputMode, Trigger}
import scala.concurrent.duration._
val q = longs.
  writeStream.
  format("console").
  trigger(Trigger.ProcessingTime(10.seconds)).
  outputMode(OutputMode.Update). // <-- required for mapGroupsWithState
  start

// Note GroupState

-------------------------------------------
Batch: 1
-------------------------------------------
>>> key: 0 => state: GroupState(<undefined>)
>>> key: 1 => state: GroupState(<undefined>)
+-----+
|value|
+-----+
|    0|
|    1|
+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
>>> key: 0 => state: GroupState(0)
>>> key: 1 => state: GroupState(0)
+-----+
|value|
+-----+
|    0|
|    1|
+-----+

-------------------------------------------
Batch: 3
-------------------------------------------
>>> key: 0 => state: GroupState(4)
>>> key: 1 => state: GroupState(4)
+-----+
|value|
+-----+
|    0|
|    1|
+-----+

// in the end
spark.streams.active(0).stop
----
