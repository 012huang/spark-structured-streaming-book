== [[Source]] Streaming Source

A *Streaming Source* represents a continuous stream of data for a streaming query. It generates batches of link:spark-sql-dataframe.adoc[DataFrame] for given start and end offsets. For fault tolerance, a source must be able to replay data given a start offset.

A streaming source should be able to replay an arbitrary sequence of past data in a stream using a range of offsets. Streaming sources like Apache Kafka and Amazon Kinesis (with their per-record offsets) fit into this model nicely. This is the assumption so structured streaming can achieve end-to-end exactly-once guarantees.

A streaming source is described by <<contract, Source contract>> in `org.apache.spark.sql.execution.streaming` package.

[source, scala]
----
import org.apache.spark.sql.execution.streaming.Source
----

There are the following `Source` implementations available:

. link:spark-sql-streaming-FileStreamSource.adoc[FileStreamSource]

. link:spark-sql-streaming-KafkaSource.adoc[KafkaSource]

. link:spark-sql-streaming-MemoryStream.adoc[MemoryStream]

. link:spark-sql-streaming-TextSocketSource.adoc[TextSocketSource]

=== [[contract]] Source Contract

[source, scala]
----
trait Source {
  def schema: StructType
  def getOffset: Option[Offset]
  def getBatch(start: Option[Offset], end: Offset): DataFrame
  def commit(end: Offset) : Unit = {}
  def stop(): Unit
}
----

.Source Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[getBatch]] `getBatch`
| Used when...
|===
