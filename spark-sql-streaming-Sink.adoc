== [[Sink]] Streaming Sink

`Sink` <<contract, adds batches>> (of records) to a storage (while writing streaming datasets).

[[available-implementations]]
.Sinks
[width="100%",cols="1,2",options="header"]
|===
| Format / Operator
| Sink

| `console`
| link:spark-sql-streaming-ConsoleSink.adoc[ConsoleSink]

a| Any `FileFormat`

* `csv`
* `hive`
* `json`
* `libsvm`
* `orc`
* `parquet`
* `text`
| link:spark-sql-streaming-FileStreamSink.adoc[FileStreamSink]

| link:spark-sql-streaming-DataStreamWriter.adoc#foreach[foreach] operator
| link:spark-sql-streaming-ForeachSink.adoc[ForeachSink]

| `kafka`
| link:spark-sql-streaming-KafkaSink.adoc[KafkaSink]

| `memory`
| link:spark-sql-streaming-MemorySink.adoc[MemorySink]
|===

TIP: You can create your own streaming format implementing link:spark-sql-streaming-StreamSinkProvider.adoc[StreamSinkProvider].

=== [[contract]] Sink Contract

[source, scala]
----
package org.apache.spark.sql.execution.streaming

trait Sink {
  def addBatch(batchId: Long, data: DataFrame): Unit
}
----

.Sink Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[addBatch]] `addBatch`
| Adds a batch of data to this sink, i.e. adds a `DataFrame` for `batchId` batch.

Used when `StreamExecution` link:spark-sql-streaming-StreamExecution.adoc#runBatch[runs a batch].
|===
