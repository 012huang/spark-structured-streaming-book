== [[KafkaSource]] KafkaSource

`KafkaSource` is a streaming link:spark-sql-streaming-source.adoc[Source] to continuously read data from Apache Kafka.

`KafkaSource` <<creating-instance, is created>> for *kafka* format (that is managed by link:spark-sql-streaming-KafkaSourceProvider.adoc[KafkaSourceProvider]).

[source, scala]
----
spark.readStream.format("kafka")
----

[NOTE]
====
Structured Streaming support for Kafka is in link:spark-sql-streaming-KafkaSourceProvider.adoc#spark-sql-kafka-0-10[`spark-sql-kafka-0-10` library dependency].

`spark-sql-kafka-0-10` module is not included by default so you have to start `spark-submit` (and "derivatives" like `spark-shell`) with link:spark-submit.adoc#packages[`--packages` command-line option].

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0
```
====

[source, scala]
----
/**
  ./bin/kafka-console-producer.sh \
    --topic topic1 \
    --broker-list localhost:9092 \
    --property parse.key=true \
    --property key.separator=,
*/
import org.apache.spark.sql.streaming.ProcessingTime
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.OutputMode
val fromKafkaTopic1ToConsole = spark.readStream
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("startingoffsets", "earliest")  // latest, earliest or JSON with {"topicA":{"part":offset,"p1":-1},"topicB":{"0":-2}}
  .load
  .select($"key" cast "string", $"value" cast "string")
  .as[(String, String)]
  .writeStream
  .trigger(ProcessingTime(2.seconds))
  .queryName("from-kafka-to-console")
  .outputMode(OutputMode.Append)
  .format("console")
  .start

fromKafkaTopic1ToConsole.stop
----

=== [[creating-instance]] Creating KafkaSource Instance

`KafkaSource` takes the following when created:

* [[sqlContext]] link:spark-sql-sqlcontext.adoc[SQLContext]
* [[kafkaReader]] `KafkaOffsetReader`
* [[executorKafkaParams]] Parameters of executors (reading from Kafka)
* [[sourceOptions]] Collection of key-value options
* [[metadataPath]] The path to metadata...FIXME
* [[startingOffsets]] `KafkaOffsetRangeLimit`
* [[failOnDataLoss]] Flag whether to...FIXME

`KafkaSource` initializes the <<internal-registries, internal registries and counters>>.
