== [[KafkaSource]] KafkaSource

`KafkaSource` is a streaming link:spark-sql-streaming-source.adoc[Source] to _"continuously"_ read data from Apache Kafka.

NOTE: _continuously_ is to mean that Kafka topics are checked for new records every link:spark-sql-streaming-trigger.adoc[trigger] (and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them).

`KafkaSource` <<creating-instance, is created>> for *kafka* format (that is registered by link:spark-sql-streaming-KafkaSourceProvider.adoc[KafkaSourceProvider]).

[source, scala]
----
spark.readStream.format("kafka")
----

[NOTE]
====
Structured Streaming support for Kafka is in a separate link:spark-sql-streaming-KafkaSourceProvider.adoc#spark-sql-kafka-0-10[spark-sql-kafka-0-10 module] (aka _library dependency_).

`spark-sql-kafka-0-10` module is not included by default so you have to start `spark-submit` (and "derivatives" like `spark-shell`) with link:spark-submit.adoc#packages[--packages command-line option].

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:{{ book.spark.version }}
```
====

:sourcedir: src/main/scala

[source,scala,indent=0]
----
include::{sourcedir}/KafkaSourceExample.scala[]
----

=== [[creating-instance]] Creating KafkaSource Instance

`KafkaSource` takes the following when created:

* [[sqlContext]] link:spark-sql-sqlcontext.adoc[SQLContext]
* [[kafkaReader]] `KafkaOffsetReader`
* [[executorKafkaParams]] Parameters of executors (reading from Kafka)
* [[sourceOptions]] Collection of key-value options
* [[metadataPath]] The path to metadata...FIXME
* [[startingOffsets]] `KafkaOffsetRangeLimit`
* [[failOnDataLoss]] Flag whether to...FIXME

`KafkaSource` initializes the <<internal-registries, internal registries and counters>>.
