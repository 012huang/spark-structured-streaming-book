== [[KafkaSource]] KafkaSource

`KafkaSource` is a link:spark-sql-streaming-Source.adoc[streaming source] that <<getBatch, generates DataFrames of records from topics in Apache Kafka>>.

NOTE: Kafka topics are checked for new records every link:spark-sql-streaming-Trigger.adoc[trigger] and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them.

[NOTE]
====
Structured Streaming support for Kafka is in a separate link:spark-sql-streaming-KafkaSourceProvider.adoc#spark-sql-kafka-0-10[spark-sql-kafka-0-10 module] (aka _library dependency_).

`spark-sql-kafka-0-10` module is not included by default so you have to start `spark-submit` (and "derivatives" like `spark-shell`) with `--packages` command-line option to "install" it.

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.1
```
====

`KafkaSource` <<creating-instance, is created>> for *kafka* format (that is registered by link:spark-sql-streaming-KafkaSourceProvider.adoc[KafkaSourceProvider]).

[source, scala]
----
val kafkaSource = spark.
  readStream.
  format("kafka"). // <-- use KafkaSource
  load
----

.KafkaSource Is Created for kafka Format by KafkaSourceProvider
image::images/KafkaSource-creating-instance.png[align="center"]

[[options]]
.KafkaSource's Options
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Default Value
| Description

| [[kafkaConsumer.pollTimeoutMs]] `kafkaConsumer.pollTimeoutMs`
|
|

| [[maxOffsetsPerTrigger]] `maxOffsetsPerTrigger`
| (empty)
| Maximum number of offsets to fetch per trigger.

If not defined, `KafkaSource` uses <<kafkaReader, KafkaOffsetReader>> to link:spark-sql-streaming-KafkaOffsetReader.adoc#fetchLatestOffsets[fetch the latest available offsets].

| `subscribe`
|
a| Topic subscription strategy that...FIXME

NOTE: Exactly one topic subscription strategy is allowed (that `KafkaSourceProvider` link:spark-sql-streaming-KafkaSourceProvider.adoc#validateGeneralOptions[validates] before creating `KafkaSource`).

| `subscribepattern`
|
a| Topic subscription strategy that...FIXME

NOTE: Exactly one topic subscription strategy is allowed (that `KafkaSourceProvider` link:spark-sql-streaming-KafkaSourceProvider.adoc#validateGeneralOptions[validates] before creating `KafkaSource`).

| `assign`
|
a| Topic subscription strategy that...FIXME

NOTE: Exactly one topic subscription strategy is allowed (that `KafkaSourceProvider` link:spark-sql-streaming-KafkaSourceProvider.adoc#validateGeneralOptions[validates] before creating `KafkaSource`).
|===

[source, scala]
----
/**
  ./bin/kafka-console-producer.sh \
    --topic topic1 \
    --broker-list localhost:9092 \
    --property parse.key=true \
    --property key.separator=,
*/
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val kafkaSource = spark.
  readStream.
  format("kafka").
  option("subscribepattern", "topic*").
  option("kafka.bootstrap.servers", "localhost:9092").
  option("startingoffsets", "earliest").  // latest, earliest or JSON with {"topicA":{"part":offset,"p1":-1},"topicB":{"0":-2}}
  load

scala> kafkaSource.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

val fromKafkaTopic1ToConsole = kafkaSource.
  select('key cast "string", 'value cast "string"). // deserialize keys and values
  as[(String, String)].
  writeStream.
  format("console").
  option("truncate", false).
  trigger(Trigger.ProcessingTime("10 seconds")).
  queryName("from-kafka-to-console").
  outputMode(OutputMode.Append).
  start

// ...after some time
fromKafkaTopic1ToConsole.stop
----

[[schema]]
`KafkaSource` uses a predefined schema that cannot be changed.

[source, scala]
----
val schema = kafkaSource.schema
scala> println(schema.treeString)
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
----

.KafkaSource's Dataset Schema (in the positional order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Type

| `key`
| `BinaryType`

| `value`
| `BinaryType`

| `topic`
| `StringType`

| `partition`
| `IntegerType`

| `offset`
| `LongType`

| `timestamp`
| `TimestampType`

| `timestampType`
| `IntegerTyp`
|===

[source, scala]
----
val topic1 = spark
  .read // <-- read one batch only
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load
scala> topic1.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
----

[[internal-registries]]
.KafkaSource's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[currentPartitionOffsets]] `currentPartitionOffsets`
| Current partition offsets (as `Map[TopicPartition, Long]`)

Initially `NONE` and set when `KafkaSource` is requested to <<getOffset, get the maximum available offsets>> or <<getBatch, generate a DataFrame with records from Kafka for a batch>>.

| [[initialPartitionOffsets]] `initialPartitionOffsets`
a| Initial partition offsets (as `Map[TopicPartition, Long]`)

Set when `KafkaSource` is first requested to <<getOffset, get the available offsets>> (from metadata log or Kafka directly).

Used when `KafkaSource` is requested to <<getBatch, generate a DataFrame with records from Kafka for a streaming batch>> (when the start offset is not defined).

---

CAUTION: FIXME When could the start offset be not defined?

---

While being initialized, `initialPartitionOffsets` link:spark-sql-streaming-HDFSMetadataLog.adoc#creating-instance[creates a HDFSMetadataLog] (with link:spark-sql-streaming-KafkaSourceOffset.adoc[KafkaSourceOffset]) and takes the ``0``th batch's metadata (as `KafkaSourceOffset`) if available.

Otherwise, if the ``0``th batch's metadata is not available, `initialPartitionOffsets` uses <<kafkaReader, KafkaOffsetReader>> to fetch offsets per <<startingOffsets, KafkaOffsetRangeLimit>> input parameter.

You should see the following INFO message in the logs:

```
INFO Initial offsets: [offsets]
```

NOTE: The ``0``th batch is persisted in the streaming metadata log unless stored already.
|===

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.sql.kafka010.KafkaSource` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=DEBUG
```

Refer to link:spark-sql-streaming-logging.adoc[Logging].
====

=== [[rateLimit]] `rateLimit` Internal Method

[source, scala]
----
rateLimit(
  limit: Long,
  from: Map[TopicPartition, Long],
  until: Map[TopicPartition, Long]): Map[TopicPartition, Long]
----

`rateLimit` requests <<kafkaReader, KafkaOffsetReader>> to link:spark-sql-streaming-KafkaOffsetReader.adoc#fetchEarliestOffsets[fetchEarliestOffsets].

CAUTION: FIXME

NOTE: `rateLimit` is used exclusively when `KafkaSource` <<getOffset, gets available offsets>> (when <<maxOffsetsPerTrigger, maxOffsetsPerTrigger>> option is specified).

=== [[getSortedExecutorList]] `getSortedExecutorList` Method

CAUTION: FIXME

=== [[reportDataLoss]] `reportDataLoss` Internal Method

CAUTION: FIXME

[NOTE]
====
`reportDataLoss` is used when `KafkaSource` does the following:

* <<fetchAndVerify, fetches and verifies specific offsets>>
* <<getBatch, generates a DataFrame with records from Kafka for a batch>>
====

=== [[getBatch]] Generating DataFrame with Records From Kafka for Streaming Batch -- `getBatch` Method

[source, scala]
----
getBatch(start: Option[Offset], end: Offset): DataFrame
----

NOTE: `getBatch` is a part of link:spark-sql-streaming-Source.adoc#getBatch[Source Contract].

`getBatch` initializes <<initialPartitionOffsets, initial partition offsets>> (unless initialized already).

You should see the following INFO message in the logs:

```
INFO KafkaSource: GetBatch called with start = [start], end = [end]
```

`getBatch` requests `KafkaSourceOffset` for link:spark-sql-streaming-KafkaSourceOffset.adoc#getPartitionOffsets[end partition offsets] for the input `end` offset (known as `untilPartitionOffsets`).

`getBatch` requests `KafkaSourceOffset` for link:spark-sql-streaming-KafkaSourceOffset.adoc#getPartitionOffsets[start partition offsets] for the input `start` offset (if defined) or uses <<initialPartitionOffsets, initial partition offsets>> (known as `fromPartitionOffsets`).

`getBatch` finds the new partitions (as the difference between the topic partitions in `untilPartitionOffsets` and `fromPartitionOffsets`) and requests <<kafkaReader, KafkaOffsetReader>> to link:spark-sql-streaming-KafkaOffsetReader.adoc#fetchEarliestOffsets[fetch their earliest offsets].

`getBatch` <<reportDataLoss, reports a data loss>> if the new partitions don't match to what <<kafkaReader, KafkaOffsetReader>> fetched.

```
Cannot find earliest offsets of [partitions]. Some data may have been missed
```

You should see the following INFO message in the logs:

```
INFO KafkaSource: Partitions added: [partitionOffsets]
```

`getBatch` <<reportDataLoss, reports a data loss>> if the new partitions don't have their offsets `0`.

```
Added partition [partition] starts from [offset] instead of 0. Some data may have been missed
```

`getBatch` <<reportDataLoss, reports a data loss>> if the `fromPartitionOffsets` partitions differ from `untilPartitionOffsets` partitions.

```
[partitions] are gone. Some data may have been missed
```

You should see the following DEBUG message in the logs:

```
DEBUG KafkaSource: TopicPartitions: [comma-separated topicPartitions]
```

`getBatch` <<getSortedExecutorList, gets the executors>> (sorted by `executorId` and `host` of the registered block managers).

IMPORTANT: That is when `getBatch` goes very low-level to allow for cached `KafkaConsumers` in the executors to be re-used to read the same partition in every batch (aka _location preference_).

You should see the following DEBUG message in the logs:

```
DEBUG KafkaSource: Sorted executors: [comma-separated sortedExecutors]
```

`getBatch` creates a `KafkaSourceRDDOffsetRange` per `TopicPartition`.

`getBatch` filters out `KafkaSourceRDDOffsetRanges` for which until offsets are smaller than from offsets. `getBatch` <<reportDataLoss, reports a data loss>> if they are found.

```
Partition [topicPartition]'s offset was changed from [fromOffset] to [untilOffset], some data may have been missed
```

`getBatch` link:spark-sql-streaming-KafkaSourceRDD.adoc#creating-instance[creates a KafkaSourceRDD] (with <<executorKafkaParams, executorKafkaParams>>, <<pollTimeoutMs, pollTimeoutMs>> and `reuseKafkaConsumer` flag enabled) and maps it to an RDD of `InternalRow`.

IMPORTANT: `getBatch` creates a `KafkaSourceRDD` with `reuseKafkaConsumer` flag enabled.

You should see the following INFO message in the logs:

```
INFO KafkaSource: GetBatch generating RDD of offset range: [comma-separated offsetRanges sorted by topicPartition]
```

`getBatch` sets <<currentPartitionOffsets, currentPartitionOffsets>> if it was empty (which is when...FIXME)

In the end, `getBatch` creates a `DataFrame` from the RDD of `InternalRow` and <<schema, schema>>.

=== [[getOffset]] Fetching Offsets (From Metadata Log or Kafka Directly) -- `getOffset` Method

[source, scala]
----
getOffset: Option[Offset]
----

NOTE: `getOffset` is a part of the link:spark-sql-streaming-Source.adoc#getOffset[Source Contract].

Internally, `getOffset` fetches the <<initialPartitionOffsets, initial partition offsets>> (from the metadata log or Kafka directly).

.KafkaSource Initializing initialPartitionOffsets While Fetching Initial Offsets
image::images/KafkaSource-initialPartitionOffsets.png[align="center"]

NOTE: <<initialPartitionOffsets, initialPartitionOffsets>> is a lazy value and is initialized the very first time `getOffset` is called (which is when `StreamExecution` link:spark-sql-streaming-StreamExecution.adoc#constructNextBatch-hasNewData[constructs the next streaming batch]).

CAUTION: FIXME Examples of 1) reading offsets from the metadata log and 2) Kafka directly.

`getOffset` requests <<kafkaReader, KafkaOffsetReader>> to link:spark-sql-streaming-KafkaOffsetReader.adoc#fetchLatestOffsets[fetchLatestOffsets] (known later as `latest`).

`getOffset` then calculates <<currentPartitionOffsets, currentPartitionOffsets>> being offsets per <<maxOffsetsPerTrigger, maxOffsetsPerTrigger>>.

.getOffset's Offset Calculation per maxOffsetsPerTrigger
[cols="1,1",options="header",width="100%"]
|===
| maxOffsetsPerTrigger
| Offsets

| Unspecified (i.e. `None`)
| `latest`

| Defined with <<currentPartitionOffsets, currentPartitionOffsets>> undefined
| <<rateLimit, rateLimit>> with `limit` limit, <<initialPartitionOffsets, initialPartitionOffsets>> as `from`, `until` as `latest`

| Defined with <<currentPartitionOffsets, currentPartitionOffsets>> defined
| <<rateLimit, rateLimit>> with `limit` limit, <<initialPartitionOffsets, initialPartitionOffsets>> as `from`, `until` as `latest`
|===

`getOffset` sets <<currentPartitionOffsets, currentPartitionOffsets>> as the `offsets` calculated above.

You should see the following DEBUG message in the logs:

```
DEBUG KafkaSource: GetOffset: [offsets]
```

In the end, `getOffset` creates a link:spark-sql-streaming-KafkaSourceOffset.adoc[KafkaSourceOffset] with `offsets`.

=== [[creating-instance]] Creating KafkaSource Instance

`KafkaSource` takes the following when created:

* [[sqlContext]] link:spark-sql-sqlcontext.adoc[SQLContext]
* [[kafkaReader]] link:spark-sql-streaming-KafkaOffsetReader.adoc[KafkaOffsetReader]
* [[executorKafkaParams]] Parameters of executors (reading from Kafka)
* [[sourceOptions]] Collection of key-value options
* [[metadataPath]] `metadataPath` -- streaming metadata log directory where `KafkaSource` persists link:spark-sql-streaming-KafkaSourceOffset.adoc[KafkaSourceOffset] offsets in JSON format.
* [[startingOffsets]] `KafkaOffsetRangeLimit`
* [[failOnDataLoss]] Flag used to link:spark-sql-streaming-KafkaSourceRDD.adoc#creating-instance[create `KafkaSourceRDDs`] every trigger and when checking to <<reportDataLoss, report a `IllegalStateException` on data loss>>.

`KafkaSource` initializes the <<internal-registries, internal registries and counters>>.

=== [[fetchAndVerify]] Fetching and Verifying Specific Offsets -- `fetchAndVerify` Internal Method

[source, scala]
----
fetchAndVerify(specificOffsets: Map[TopicPartition, Long]): KafkaSourceOffset
----

`fetchAndVerify` requests <<kafkaReader, KafkaOffsetReader>> to link:spark-sql-streaming-KafkaOffsetReader.adoc#fetchSpecificOffsets[fetchSpecificOffsets] for the given `specificOffsets`.

`fetchAndVerify` makes sure that the starting offsets in `specificOffsets` are the same as in Kafka and <<reportDataLoss, reports a data loss>> otherwise.

```
startingOffsets for [tp] was [off] but consumer reset to [result(tp)]
```

In the end, `fetchAndVerify` creates a link:spark-sql-streaming-KafkaSourceOffset.adoc[KafkaSourceOffset] (with the result of <<kafkaReader, KafkaOffsetReader>>).

NOTE: `fetchAndVerify` is used exclusively when `KafkaSource` initializes <<initialPartitionOffsets, initial partition offsets>>.
