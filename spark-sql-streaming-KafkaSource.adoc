== [[KafkaSource]] KafkaSource

`KafkaSource` is a streaming link:spark-sql-streaming-source.adoc[Source] to _"continuously"_ read data from Apache Kafka.

NOTE: _continuously_ is to mean that Kafka topics are checked for new records every link:spark-sql-streaming-trigger.adoc[trigger] (and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them).

`KafkaSource` <<creating-instance, is created>> for *kafka* format (that is registered by link:spark-sql-streaming-KafkaSourceProvider.adoc[KafkaSourceProvider]).

[source, scala]
----
spark.readStream.format("kafka")
----

[NOTE]
====
Structured Streaming support for Kafka is in a separate link:spark-sql-streaming-KafkaSourceProvider.adoc#spark-sql-kafka-0-10[spark-sql-kafka-0-10 module] (aka _library dependency_).

`spark-sql-kafka-0-10` module is not included by default so you have to start `spark-submit` (and "derivatives" like `spark-shell`) with link:spark-submit.adoc#packages[--packages command-line option].

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0
```
====

[source, scala]
----
/**
  ./bin/kafka-console-producer.sh \
    --topic topic1 \
    --broker-list localhost:9092 \
    --property parse.key=true \
    --property key.separator=,
*/
import org.apache.spark.sql.streaming.ProcessingTime
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.OutputMode
val fromKafkaTopic1ToConsole = spark.readStream
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("startingoffsets", "earliest")  // latest, earliest or JSON with {"topicA":{"part":offset,"p1":-1},"topicB":{"0":-2}}
  .load
  .select($"key" cast "string", $"value" cast "string") // deserialize records
  .as[(String, String)]
  .writeStream
  .trigger(ProcessingTime(2.seconds))
  .queryName("from-kafka-to-console")
  .outputMode(OutputMode.Append)
  .format("console")
  .start

fromKafkaTopic1ToConsole.stop
----

[[internal-registries]]
.KafkaSource's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[initialPartitionOffsets]] `initialPartitionOffsets`
| FIXME

Used when...FIXME
|===

=== [[getBatch]] `getBatch` Method

CAUTION: FIXME

NOTE: `getBatch` is a part of link:spark-sql-streaming-source.adoc#getBatch[Source Contract] to...FIXME

=== [[fetchAndVerify]] `fetchAndVerify` Internal Method

CAUTION: FIXME

NOTE: `fetchAndVerify` is used exclusively when `KafkaSource` <<creating-instance, is created>> (namely when <<initialPartitionOffsets, initialPartitionOffsets>> is initialized).

=== [[reportDataLoss]] `reportDataLoss` Internal Method

CAUTION: FIXME

[NOTE]
====
`reportDataLoss` is used when `KafkaSource` does the following:

* <<fetchAndVerify, fetchAndVerify>>
* <<getBatch, getBatch>>
====

=== [[creating-instance]] Creating KafkaSource Instance

`KafkaSource` takes the following when created:

* [[sqlContext]] link:spark-sql-sqlcontext.adoc[SQLContext]
* [[kafkaReader]] `KafkaOffsetReader`
* [[executorKafkaParams]] Parameters of executors (reading from Kafka)
* [[sourceOptions]] Collection of key-value options
* [[metadataPath]] The path to metadata...FIXME
* [[startingOffsets]] `KafkaOffsetRangeLimit`
* [[failOnDataLoss]] Flag used to link:spark-sql-streaming-KafkaSourceRDD.adoc#creating-instance[create `KafkaSourceRDDs`] every trigger and when checking to <<reportDataLoss, report a `IllegalStateException` on data loss>>.

`KafkaSource` initializes the <<internal-registries, internal registries and counters>>.
