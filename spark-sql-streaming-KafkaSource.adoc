== [[KafkaSource]] KafkaSource

`KafkaSource` is a link:spark-sql-streaming-Source.adoc[streaming source] to _"continuously"_ read datasets from Apache Kafka.

NOTE: _"continuously"_ means that Kafka topics are checked for new records every link:spark-sql-streaming-Trigger.adoc[trigger] (and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them).

`KafkaSource` <<creating-instance, is created>> for *kafka* format (that is registered by link:spark-sql-streaming-KafkaSourceProvider.adoc[KafkaSourceProvider]).

[source, scala]
----
spark.readStream.format("kafka")
----

[NOTE]
====
Structured Streaming support for Kafka is in a separate link:spark-sql-streaming-KafkaSourceProvider.adoc#spark-sql-kafka-0-10[spark-sql-kafka-0-10 module] (aka _library dependency_).

`spark-sql-kafka-0-10` module is not included by default so you have to start `spark-submit` (and "derivatives" like `spark-shell`) with `--packages` command-line option.

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.1
```
====

[source, scala]
----
/**
  ./bin/kafka-console-producer.sh \
    --topic topic1 \
    --broker-list localhost:9092 \
    --property parse.key=true \
    --property key.separator=,
*/
import org.apache.spark.sql.streaming.ProcessingTime
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.OutputMode
val fromKafkaTopic1ToConsole = spark.readStream
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("startingoffsets", "earliest")  // latest, earliest or JSON with {"topicA":{"part":offset,"p1":-1},"topicB":{"0":-2}}
  .load
  .select('key cast "string", 'value cast "string") // deserialize records
  .as[(String, String)]
  .writeStream
  .option("checkpointLocation", "target/kafka-checkpoint") // used for keeping state of writing side
  .trigger(ProcessingTime(2.seconds))
  .queryName("from-kafka-to-console")
  .outputMode(OutputMode.Append)
  .format("console")
  .start

fromKafkaTopic1ToConsole.stop
----

[[schema]]
The schema of datasets is predefined and cannot be changed.

.KafkaSource's Dataset Schema (in the positional order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Type

| `key`
| `BinaryType`

| `value`
| `BinaryType`

| `topic`
| `StringType`

| `partition`
| `IntegerType`

| `offset`
| `LongType`

| `timestamp`
| `TimestampType`

| `timestampType`
| `IntegerTyp`
|===

[source, scala]
----
val topic1 = spark
  .read
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load
scala> topic1.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
----

[[internal-registries]]
.KafkaSource's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[initialPartitionOffsets]] `initialPartitionOffsets`
a| Kafka's `TopicPartitions` with offsets.

Lazily set when `KafkaSource` <<getOffset, gets maximum available offset>> and used for <<getBatch, fetching records for a batch>>.

When initialized, `initialPartitionOffsets` creates a `HDFSMetadataLog` (for `KafkaSourceOffset`) and takes the 0th batch's metadata (as `KafkaSourceOffset`) if available. If not, it uses <<kafkaReader, KafkaOffsetReader>> to fetch offsets per <<startingOffsets, KafkaOffsetRangeLimit>> input parameter.

You should see the following INFO message in the logs:

```
INFO Initial offsets: [offsets]
```

NOTE: The 0th batch is persisted in the streaming metadata log if not available initially.

|===

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.sql.kafka010.KafkaSource` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=DEBUG
```

Refer to link:spark-sql-streaming-logging.adoc[Logging].
====

=== [[rateLimit]] `rateLimit` Internal Method

CAUTION: FIXME

=== [[getBatch]] Fetching Records for Batch -- `getBatch` Method

[source, scala]
----
getBatch(start: Option[Offset], end: Offset): DataFrame
----

NOTE: `getBatch` is a part of link:spark-sql-streaming-Source.adoc#getBatch[Source Contract].

=== [[fetchAndVerify]] `fetchAndVerify` Internal Method

CAUTION: FIXME

NOTE: `fetchAndVerify` is used exclusively when `KafkaSource` reads <<initialPartitionOffsets, initialPartitionOffsets>>.

=== [[reportDataLoss]] `reportDataLoss` Internal Method

CAUTION: FIXME

[NOTE]
====
`reportDataLoss` is used when `KafkaSource` does the following:

* <<fetchAndVerify, fetchAndVerify>>
* <<getBatch, getBatch>>
====

=== [[getOffset]] Getting Maximum Available Offsets -- `getOffset` Method

[source, scala]
----
getOffset: Option[Offset]
----

Internally, `getOffset` <<initialPartitionOffsets, initializes partition offsets>>.

NOTE: <<initialPartitionOffsets, initialPartitionOffsets>> is a lazy value and is initialized the very first time `getOffset` is called.

`getOffset` requests <<kafkaReader, KafkaOffsetReader>> to link:spark-sql-streaming-KafkaOffsetReader.adoc#fetchLatestOffsets[fetchLatestOffsets] (known later as `latest`).

`getOffset` then calculates <<currentPartitionOffsets, currentPartitionOffsets>> being offsets per <<maxOffsetsPerTrigger, maxOffsetsPerTrigger>>.

.getOffset's Offset Calculation per maxOffsetsPerTrigger
[cols="1,1",options="header",width="100%"]
|===
| maxOffsetsPerTrigger
| Offsets

| Unspecified (i.e. `None`)
| `latest`

| Defined with <<currentPartitionOffsets, currentPartitionOffsets>> undefined
| <<rateLimit, rateLimit>> with `limit` limit, <<initialPartitionOffsets, initialPartitionOffsets>> as `from`, `until` as `latest`

| Defined with <<currentPartitionOffsets, currentPartitionOffsets>> defined
| <<rateLimit, rateLimit>> with `limit` limit, <<initialPartitionOffsets, initialPartitionOffsets>> as `from`, `until` as `latest`
|===

`getOffset` sets <<currentPartitionOffsets, currentPartitionOffsets>> as the `offsets` calculated above.

You should see the following DEBUG message in the logs:

```
DEBUG KafkaSource: GetOffset: [offsets]
```

In the end, `getOffset` creates a `KafkaSourceOffset` with `offsets`.

NOTE: `getOffset` is a part of link:spark-sql-streaming-Source.adoc#getOffset[Source Contract].

=== [[creating-instance]] Creating KafkaSource Instance

`KafkaSource` takes the following when created:

* [[sqlContext]] link:spark-sql-sqlcontext.adoc[SQLContext]
* [[kafkaReader]] link:spark-sql-streaming-KafkaOffsetReader.adoc[KafkaOffsetReader]
* [[executorKafkaParams]] Parameters of executors (reading from Kafka)
* [[sourceOptions]] Collection of key-value options
* [[metadataPath]] `metadataPath` -- streaming metadata log directory where `KafkaSource` persists `KafkaSourceOffset` offsets in JSON format.
* [[startingOffsets]] `KafkaOffsetRangeLimit`
* [[failOnDataLoss]] Flag used to link:spark-sql-streaming-KafkaSourceRDD.adoc#creating-instance[create `KafkaSourceRDDs`] every trigger and when checking to <<reportDataLoss, report a `IllegalStateException` on data loss>>.

`KafkaSource` initializes the <<internal-registries, internal registries and counters>>.
