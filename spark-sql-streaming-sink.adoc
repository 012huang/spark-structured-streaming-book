== [[Sink]] Streaming Sink

`Sink` <<contract, adds batches>> (of records) to a storage (while writing streaming datasets).

[[sinks]]
.Sinks (in alphabetical order)
[width="100%",cols="1,2",options="header"]
|===
| Format / Operator
| Name

| `console`
| link:spark-sql-streaming-ConsoleSink.adoc[ConsoleSink]

a| Any `FileFormat`

* `csv`
* `hive`
* `json`
* `libsvm`
* `orc`
* `parquet`
* `text`
| link:spark-sql-streaming-FileStreamSink.adoc[FileStreamSink]

| link:spark-sql-streaming-DataStreamWriter.adoc#foreach[foreach] operator
| link:spark-sql-streaming-ForeachSink.adoc[ForeachSink]

| `kafka`
| link:spark-sql-streaming-KafkaSink.adoc[KafkaSink]

| `memory`
| link:spark-sql-streaming-MemorySink.adoc[MemorySink]
|===

TIP: You can create your own streaming format implementing link:spark-sql-streaming-StreamSinkProvider.adoc[StreamSinkProvider].

=== [[contract]] Sink Contract

[source, scala]
----
package org.apache.spark.sql.execution.streaming

trait Sink {
  def addBatch(batchId: Long, data: DataFrame): Unit
}
----

.Sink Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[addBatch]] `addBatch`
| Used when...
|===
