== [[Source]] Streaming Source

A *Streaming Source* represents a continuous stream of data for a streaming query. It generates batches of link:spark-sql-dataframe.adoc[DataFrame] for given start and end offsets. For fault tolerance, a source must be able to replay data given a start offset.

A streaming source should be able to replay an arbitrary sequence of past data in a stream using a range of offsets. Streaming sources like Apache Kafka and Amazon Kinesis (with their per-record offsets) fit into this model nicely. This is the assumption so structured streaming can achieve end-to-end exactly-once guarantees.

A streaming source is described by <<contract, Source contract>> in `org.apache.spark.sql.execution.streaming` package.

[source, scala]
----
import org.apache.spark.sql.execution.streaming.Source
----

[[available-implementations]]
.Sources
[cols="1,2",options="header",width="100%"]
|===
| Format
| Source

a| Any `FileFormat`

* `csv`
* `hive`
* `json`
* `libsvm`
* `orc`
* `parquet`
* `text`
| link:spark-sql-streaming-FileStreamSource.adoc[FileStreamSource]

| `kafka`
| link:spark-sql-streaming-KafkaSource.adoc[KafkaSource]

| `memory`
| link:spark-sql-streaming-MemoryStream.adoc[MemoryStream]

| `rate`
| link:spark-sql-streaming-RateStreamSource.adoc[RateStreamSource]

| `socket`
| link:spark-sql-streaming-TextSocketSource.adoc[TextSocketSource]
|===

=== [[contract]] Source Contract

[source, scala]
----
trait Source {
  def schema: StructType
  def getOffset: Option[Offset]
  def getBatch(start: Option[Offset], end: Offset): DataFrame
  def commit(end: Offset) : Unit = {}
  def stop(): Unit
}
----

.Source Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[getBatch]] `getBatch`
| Generates a `DataFrame` (with new rows from a streaming source) for a given batch (described using the start and end offsets).

Used when `StreamExecution` link:spark-sql-streaming-StreamExecution.adoc#runBatch[runs a batch] and link:spark-sql-streaming-StreamExecution.adoc#populateStartOffsets[populateStartOffsets].

| [[getOffset]] `getOffset`
| Used exclusively when `StreamExecution` link:spark-sql-streaming-StreamExecution.adoc#runBatches[run batches] (while link:spark-sql-streaming-StreamExecution.adoc#constructNextBatch[constructing next batch])
|===
