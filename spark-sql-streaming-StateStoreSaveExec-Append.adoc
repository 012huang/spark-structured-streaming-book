== Demo: StateStoreSaveExec in Append Output Mode

The following example code shows the behaviour of link:spark-sql-streaming-StateStoreSaveExec.adoc#doExecute-Append[StateStoreSaveExec in Append output mode].

NOTE: Append output mode requires that a streaming query defines event time watermark (using link:spark-sql-streaming-Dataset-operators.adoc#withWatermark[withWatermark] operator) on the event time column that is used for aggregation (directly or using link:spark-sql-streaming-window.adoc[window] function).

CAUTION: FIXME UnsupportedOperationChecker makes sure that the requirement holds

NOTE: link:spark-sql-streaming-Dataset-operators.adoc#withWatermark[withWatermark] operator has to be used before the aggregation (for the watermark to be used).

NOTE: Sorting is not supported on streaming DataFrames/Datasets, unless it is on an aggregated Dataset in `Complete` output mode.

CAUTION: FIXME Review UnsupportedOperationChecker.scala:297

[source, scala]
----
// START: Only for easier debugging
// The state is then only for one partition
// which should make monitoring it easier
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1)
scala> spark.sessionState.conf.numShufflePartitions
res1: Int = 1
// END: Only for easier debugging

// Read datasets from a Kafka topic
// ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT
// Use streaming aggregation with groupBy operator to have StateStoreSaveExec operator
// Since the demo uses Append output mode
// it has to define a streaming event time watermark using withWatermark operator
// UnsupportedOperationChecker makes sure that the requirement holds
val valuesPerDevice = spark.
  readStream.
  format("kafka").
  option("subscribe", "topic1").
  option("kafka.bootstrap.servers", "localhost:9092").
  load.
  withColumn("tokens", split('value, ",")).
  withColumn("seconds", 'tokens(0) cast "long").
  withColumn("event_time", to_timestamp(from_unixtime('seconds))). // <-- Event time has to be a timestamp
  withColumn("device", 'tokens(1)).
  withColumn("level", 'tokens(2) cast "int").
  withWatermark(eventTime = "event_time", delayThreshold = "10 seconds"). // <-- define watermark (before groupBy!)
  groupBy($"event_time"). // <-- use event_time for grouping
  agg(collect_list("level") as "levels", collect_list("device") as "devices").
  withColumn("event_time", to_timestamp($"event_time")) // <-- convert to human-readable date

// valuesPerDevice is a streaming Dataset with just one Kafka source
// so it knows nothing about output mode or the current streaming watermark yet
// - Output mode is defined on writing side
// - streaming watermark is read from rows at runtime
// That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode)
// and no batch-specific values are printed out
// They will be available right after the first streaming batch
// Use explain on a streaming query to know the runtime-specific values
scala> valuesPerDevice.explain
== Physical Plan ==
*Project [event_time#36-T10000ms AS event_time#97, levels#90, devices#92]
+- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[collect_list(level#61, 0, 0), collect_list(device#48, 0, 0)])
   +- Exchange hashpartitioning(event_time#36-T10000ms, 1)
      +- StateStoreSave [event_time#36-T10000ms], StatefulOperatorStateInfo(<unknown>,755d4737-58ac-4b07-9a3f-07f66a2f1374,0,0), Append, 0
         +- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[merge_collect_list(level#61, 0, 0), merge_collect_list(device#48, 0, 0)])
            +- Exchange hashpartitioning(event_time#36-T10000ms, 1)
               +- StateStoreRestore [event_time#36-T10000ms], StatefulOperatorStateInfo(<unknown>,755d4737-58ac-4b07-9a3f-07f66a2f1374,0,0)
                  +- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[merge_collect_list(level#61, 0, 0), merge_collect_list(device#48, 0, 0)])
                     +- Exchange hashpartitioning(event_time#36-T10000ms, 1)
                        +- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[partial_collect_list(level#61, 0, 0), partial_collect_list(device#48, 0, 0)])
                           +- EventTimeWatermark event_time#36: timestamp, interval 10 seconds
                              +- *Project [cast(from_unixtime(cast(split(cast(value#1 as string), ,)[0] as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Berlin)) as timestamp) AS event_time#36, split(cast(value#1 as string), ,)[1] AS device#48, cast(split(cast(value#1 as string), ,)[2] as int) AS level#61]
                                 +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]

// Start the query and hence StateStoreSaveExec
// Note Append output mode
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val sq = valuesPerDevice.
  writeStream.
  format("console").
  option("truncate", false).
  trigger(Trigger.ProcessingTime(10.seconds)).
  outputMode(OutputMode.Append).
  start

-------------------------------------------
Batch: 0
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// there's only 1 stateful operator and hence 0 for the index in stateOperators
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 0,
  "numRowsUpdated" : 0,
  "memoryUsedBytes" : 77
}

// Current watermark
// We've just started so it's the default start time
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

// publish new records
// timestamp,device,value
// delay threshold = 10 seconds
// 1,1,1
// 2,1,2
// 3,1,3

-------------------------------------------
Batch: 1
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier (it's just started!) and so numRowsUpdated is 0
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 3,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 965
}

// Current watermark
// One streaming batch has passed so it's still the default start time
// that will get changed the next streaming batch
// watermark is always one batch behind
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

// Could be 0 if the time to update the lastProgress is short
// FIXME Explain it in detail
scala> println(sq.lastProgress.numInputRows)
1

// publish new record with event_time > current watermark + delayThreshold
// i.e. 0 + 10 = 10
// timestamp,device,value
// delay threshold = 10 seconds
// 11,1,4

-------------------------------------------
Batch: 2
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 4,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 1277
}

// Current watermark
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

scala> println(sq.lastProgress.numInputRows)
1

// publish new record with any event_time
// timestamp,device,value
// delay threshold = 10 seconds
// 10,1,5

-------------------------------------------
Batch: 4
-------------------------------------------
+-------------------+------+-------+
|event_time         |levels|devices|
+-------------------+------+-------+
|1970-01-01 01:00:01|[1]   |[1]    |
+-------------------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 4,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 1277
}

// Current watermark
// Updating eventTime watermark to: 1000 ms
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:01.000Z

scala> println(sq.lastProgress.numInputRows)
1

// publish new record with quite advanced event_time
// that will advance watermark next streaming batch
// timestamp,device,value
// delay threshold = 10 seconds
// 50,1,6

-------------------------------------------
Batch: 5
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 5,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 1557
}

// Current watermark
// Updating eventTime watermark to: 1000 ms
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:01.000Z

scala> println(sq.lastProgress.numInputRows)
1

// publish new record with any event_time but new key
// Some rows should have expired event_time given the previous event_time
// timestamp,device,value
// delay threshold = 10 seconds
// 30,2,1

-------------------------------------------
Batch: 6
-------------------------------------------
+-------------------+------+-------+
|event_time         |levels|devices|
+-------------------+------+-------+
|1970-01-01 01:00:03|[3]   |[1]    |
|1970-01-01 01:00:02|[2]   |[1]    |
|1970-01-01 01:00:11|[4]   |[1]    |
|1970-01-01 01:00:10|[5]   |[1]    |
+-------------------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 1,
  "numRowsUpdated" : 0,
  "memoryUsedBytes" : 437
}

// Current watermark
// Updating eventTime watermark to: 40000 ms
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:40.000Z

scala> println(sq.lastProgress.numInputRows)
1

// In the end...
sq.stop
----
