== Demo: StateStoreSaveExec in Append Output Mode

The following example code shows the behaviour of link:spark-sql-streaming-StateStoreSaveExec.adoc#doExecute-Append[StateStoreSaveExec in Append output mode].

NOTE: `Append` output mode requires that a streaming query defines event time watermark (using link:spark-sql-streaming-Dataset-withWatermark.adoc[withWatermark] operator) on the event time column that is used for aggregation (directly or using link:spark-sql-streaming-window.adoc[window] function).

CAUTION: FIXME UnsupportedOperationChecker makes sure that the requirement holds

NOTE: link:spark-sql-streaming-Dataset-withWatermark.adoc[withWatermark] operator has to be used before the aggregation (for the watermark to be used).

In `Append` output mode there are the following questions to ask about what to include in the output Dataset:

1. What is the current watermark level (that is used to expire state rows and ignore late events)?

1. Is the key that expired in the input?

NOTE: Sorting is not supported on streaming DataFrames/Datasets, unless it is on an aggregated Dataset in `Complete` output mode.

CAUTION: FIXME Review UnsupportedOperationChecker.scala:297

[[events]]
.Streaming Batches, Events, Watermark, Expired State and Late Events
[cols="^m,^.^1,^.^1,^.^1",options="header",width="100%"]
|===
| Batch / Events
| Event Time Watermark [ms]
| Expired State
| Late Events

a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 1 ! 1 ! 1
! 15 ! 1 ! 2
!====

^.^| *0*
|
|

a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 15 ! 1 ! 3
! 35 ! 1 ! 4
!====

^.^| *5000*

(Maximum event time `15` minus the `delayThreshold` as defined using link:spark-sql-streaming-Dataset-withWatermark.adoc[withWatermark] operator, i.e. `10`)
a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 1 ! 1 ! 1
!====

|

a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 1 ! 1 ! 5
! 1 ! 2 ! 1
! 15 ! 1 ! 6
! 15 ! 2 ! 2
! 16 ! 2 ! 3
!====

^.^| *25000*

(Maximum event time from the previous batch is `35` and `10` seconds of `delayThreshold`)
a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 15 ! 1 ! 2
! 15 ! 1 ! 3
!====

a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 1 ! 1 ! 5
! 1 ! 2 ! 1
!====

a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 36 ! 1 ! 7
!====

^.^| *25000*

(Maximum event time from the previous batch is `16`)
|
|

a|
[cols="^1 ,^1 ,^1",options="header"]
!====
! event_time ! device ! value
! 50 ! 2 ! 4
!====

^.^| *26000*

(Maximum event time from the previous batch is `36`)
|
|

|===

NOTE: Event time watermark may advance based on the maximum event time from the previous events (from the previous batch exactly as the level advances every trigger so the earlier levels are already counted in).

NOTE: Event time watermark can only change when the maximum event time is bigger than the current watermark minus the `delayThreshold` (as defined using link:spark-sql-streaming-Dataset-withWatermark.adoc[withWatermark] operator).

[TIP]
====
Use the following to publish events to Kafka.

```
// 1st streaming batch
$ cat /tmp/1
1,1,1
15,1,2

$ kafkacat -P -b localhost:9092 -t topic1 -l /tmp/1

// Alternatively (and slower due to JVM bootup)
$ cat /tmp/1 | ./bin/kafka-console-producer.sh --topic topic1 --broker-list localhost:9092
```
====

[source, scala]
----
// START: Only for easier debugging
// The state is then only for one partition
// which should make monitoring it easier
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1)
scala> spark.sessionState.conf.numShufflePartitions
res1: Int = 1
// END: Only for easier debugging

// Read datasets from a Kafka topic
// ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT
// Use streaming aggregation with groupBy operator to have StateStoreSaveExec operator
// Since the demo uses Append output mode
// it has to define a streaming event time watermark using withWatermark operator
// UnsupportedOperationChecker makes sure that the requirement holds
val valuesPerDevice = spark.
  readStream.
  format("kafka").
  option("subscribe", "topic1").
  option("kafka.bootstrap.servers", "localhost:9092").
  load.
  withColumn("tokens", split('value, ",")).
  withColumn("seconds", 'tokens(0) cast "long").
  withColumn("event_time", to_timestamp(from_unixtime('seconds))). // <-- Event time has to be a timestamp
  withColumn("device", 'tokens(1)).
  withColumn("level", 'tokens(2) cast "int").
  withWatermark(eventTime = "event_time", delayThreshold = "10 seconds"). // <-- define watermark (before groupBy!)
  groupBy($"event_time"). // <-- use event_time for grouping
  agg(collect_list("level") as "levels", collect_list("device") as "devices").
  withColumn("event_time", to_timestamp($"event_time")) // <-- convert to human-readable date

// valuesPerDevice is a streaming Dataset with just one Kafka source
// so it knows nothing about output mode or the current streaming watermark yet
// - Output mode is defined on writing side
// - streaming watermark is read from rows at runtime
// That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode)
// and no batch-specific values are printed out
// They will be available right after the first streaming batch
// Use explain on a streaming query to know the runtime-specific values
scala> valuesPerDevice.explain
== Physical Plan ==
*Project [event_time#36-T10000ms AS event_time#97, levels#90, devices#92]
+- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[collect_list(level#61, 0, 0), collect_list(device#48, 0, 0)])
   +- Exchange hashpartitioning(event_time#36-T10000ms, 1)
      +- StateStoreSave [event_time#36-T10000ms], StatefulOperatorStateInfo(<unknown>,e2c5c662-1397-467a-80f4-e7a0fc7c1fcc,0,0), Append, 0
         +- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[merge_collect_list(level#61, 0, 0), merge_collect_list(device#48, 0, 0)])
            +- Exchange hashpartitioning(event_time#36-T10000ms, 1)
               +- StateStoreRestore [event_time#36-T10000ms], StatefulOperatorStateInfo(<unknown>,e2c5c662-1397-467a-80f4-e7a0fc7c1fcc,0,0)
                  +- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[merge_collect_list(level#61, 0, 0), merge_collect_list(device#48, 0, 0)])
                     +- Exchange hashpartitioning(event_time#36-T10000ms, 1)
                        +- ObjectHashAggregate(keys=[event_time#36-T10000ms], functions=[partial_collect_list(level#61, 0, 0), partial_collect_list(device#48, 0, 0)])
                           +- EventTimeWatermark event_time#36: timestamp, interval 10 seconds
                              +- *Project [cast(from_unixtime(cast(split(cast(value#1 as string), ,)[0] as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Berlin)) as timestamp) AS event_time#36, split(cast(value#1 as string), ,)[1] AS device#48, cast(split(cast(value#1 as string), ,)[2] as int) AS level#61]
                                 +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]

// Start the query and hence StateStoreSaveExec
// Note Append output mode
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val sq = valuesPerDevice.
  writeStream.
  format("console").
  option("truncate", false).
  trigger(Trigger.ProcessingTime(5.seconds)).
  outputMode(OutputMode.Append). // <-- Append output mode
  start

-------------------------------------------
Batch: 0
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// there's only 1 stateful operator and hence 0 for the index in stateOperators
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 0,
  "numRowsUpdated" : 0,
  "memoryUsedBytes" : 77
}

// Current watermark
// We've just started so it's the default start time
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

// publish new records
// timestamp,device,value
// delay threshold = 10 seconds
// 1,1,1
// 2,1,2
// 3,1,3

-------------------------------------------
Batch: 1
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier (it's just started!) and so numRowsUpdated is 0
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 3,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 965
}

// Current watermark
// One streaming batch has passed so it's still the default start time
// that will get changed the next streaming batch
// watermark is always one batch behind
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

// Could be 0 if the time to update the lastProgress is short
// FIXME Explain it in detail
scala> println(sq.lastProgress.numInputRows)
1

// publish new record with event_time > current watermark + delayThreshold
// i.e. 0 + 10 = 10
// timestamp,device,value
// delay threshold = 10 seconds
// 11,1,4

-------------------------------------------
Batch: 2
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 4,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 1277
}

// Current watermark
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

scala> println(sq.lastProgress.numInputRows)
1

// publish new record with any event_time
// timestamp,device,value
// delay threshold = 10 seconds
// 10,1,5

-------------------------------------------
Batch: 4
-------------------------------------------
+-------------------+------+-------+
|event_time         |levels|devices|
+-------------------+------+-------+
|1970-01-01 01:00:01|[1]   |[1]    |
+-------------------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 4,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 1277
}

// Current watermark
// Updating eventTime watermark to: 1000 ms
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:01.000Z

scala> println(sq.lastProgress.numInputRows)
1

// publish new record with quite advanced event_time
// that will advance watermark next streaming batch
// timestamp,device,value
// delay threshold = 10 seconds
// 50,1,6

-------------------------------------------
Batch: 5
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 5,
  "numRowsUpdated" : 1,
  "memoryUsedBytes" : 1557
}

// Current watermark
// Updating eventTime watermark to: 1000 ms
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:01.000Z

scala> println(sq.lastProgress.numInputRows)
1

// publish new record with any event_time but new key
// Some rows should have expired event_time given the previous event_time
// timestamp,device,value
// delay threshold = 10 seconds
// 30,2,1

-------------------------------------------
Batch: 6
-------------------------------------------
+-------------------+------+-------+
|event_time         |levels|devices|
+-------------------+------+-------+
|1970-01-01 01:00:03|[3]   |[1]    |
|1970-01-01 01:00:02|[2]   |[1]    |
|1970-01-01 01:00:11|[4]   |[1]    |
|1970-01-01 01:00:10|[5]   |[1]    |
+-------------------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 1,
  "numRowsUpdated" : 0,
  "memoryUsedBytes" : 437
}

// Current watermark
// Updating eventTime watermark to: 40000 ms
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:40.000Z

scala> println(sq.lastProgress.numInputRows)
1

// In the end...
sq.stop
----
