== Demo: StateStoreSaveExec in Append Output Mode

The following example code shows the behaviour of link:spark-sql-streaming-StateStoreSaveExec.adoc#doExecute-Append[StateStoreSaveExec in Append output mode].

NOTE: Append output mode requires that a streaming query defines event time watermark (using link:spark-sql-streaming-Dataset-operators.adoc#withWatermark[withWatermark] operator) on the event time column that is used for aggregation (directly or using link:spark-sql-streaming-window.adoc[window] function).

CAUTION: FIXME UnsupportedOperationChecker makes sure that the requirement holds

NOTE: link:spark-sql-streaming-Dataset-operators.adoc#withWatermark[withWatermark] operator has to be used before the aggregation (for the watermark to be used).

NOTE: Sorting is not supported on streaming DataFrames/Datasets, unless it is on an aggregated Dataset in `Complete` output mode.

CAUTION: FIXME Review UnsupportedOperationChecker.scala:297

[source, scala]
----
// START: Only for easier debugging
// The state is then only for one partition
// which should make monitoring it easier
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1)
scala> spark.sessionState.conf.numShufflePartitions
res1: Int = 1
// END: Only for easier debugging

// Read datasets from a Kafka topic
// ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT
// Use streaming aggregation with groupBy operator to have StateStoreSaveExec operator
// Since the demo uses Append output mode
// it has to define a streaming event time watermark using withWatermark operator
// UnsupportedOperationChecker makes sure that the requirement holds
val valuesPerDevice = spark.
  readStream.
  format("kafka").
  option("subscribe", "topic1").
  option("kafka.bootstrap.servers", "localhost:9092").
  load.
  withColumn("tokens", split('value, ",")).
  withColumn("seconds", 'tokens(0) cast "long").
  withColumn("event_time", to_timestamp(from_unixtime('seconds))). // <-- Event time has to be a timestamp
  withColumn("device", 'tokens(1)).
  withColumn("level", 'tokens(2) cast "int").
  withWatermark(eventTime = "event_time", delayThreshold = "10 seconds"). // <-- define watermark (before groupBy!)
  groupBy($"event_time"). // <-- use event_time for grouping
  agg(collect_list("level") as "levels", collect_list("device") as "devices").
  withColumn("event_time", to_timestamp($"event_time")) // <-- convert to human-readable date

// valuesPerDevice is a streaming Dataset with just one Kafka source
// so it knows nothing about output mode or the current streaming watermark yet
// - Output mode is defined on writing side
// - streaming watermark is read from rows at runtime
// That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode)
// and no batch-specific values are printed out
// They will be available right after the first streaming batch
// Use explain on a streaming query to know the runtime-specific values
scala> valuesPerDevice.explain
== Physical Plan ==
*Project [cast(event_time#25-T10000ms as date) AS event_time#82, levels#75, devices#77]
+- ObjectHashAggregate(keys=[event_time#25-T10000ms], functions=[collect_list(level#48, 0, 0), collect_list(device#36, 0, 0)])
   +- Exchange hashpartitioning(event_time#25-T10000ms, 1)
      +- StateStoreSave [event_time#25-T10000ms], StatefulOperatorStateInfo(<unknown>,9c021e84-61ca-45cd-9212-92cfdc23dc77,0,0), Append, 0
         +- ObjectHashAggregate(keys=[event_time#25-T10000ms], functions=[merge_collect_list(level#48, 0, 0), merge_collect_list(device#36, 0, 0)])
            +- Exchange hashpartitioning(event_time#25-T10000ms, 1)
               +- StateStoreRestore [event_time#25-T10000ms], StatefulOperatorStateInfo(<unknown>,9c021e84-61ca-45cd-9212-92cfdc23dc77,0,0)
                  +- ObjectHashAggregate(keys=[event_time#25-T10000ms], functions=[merge_collect_list(level#48, 0, 0), merge_collect_list(device#36, 0, 0)])
                     +- Exchange hashpartitioning(event_time#25-T10000ms, 1)
                        +- ObjectHashAggregate(keys=[event_time#25-T10000ms], functions=[partial_collect_list(level#48, 0, 0), partial_collect_list(device#36, 0, 0)])
                           +- EventTimeWatermark event_time#25: timestamp, interval 10 seconds
                              +- *Project [cast(split(cast(value#1 as string), ,)[0] as timestamp) AS event_time#25, split(cast(value#1 as string), ,)[1] AS device#36, cast(split(cast(value#1 as string), ,)[2] as int) AS level#48]
                                 +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]

// Start the query and hence StateStoreSaveExec
// Note Append output mode
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val sq = valuesPerDevice.
  writeStream.
  format("console").
  option("truncate", false).
  trigger(Trigger.ProcessingTime(10.seconds)).
  outputMode(OutputMode.Append).
  start

-------------------------------------------
Batch: 0
-------------------------------------------
+----------+------+-------+
|event_time|levels|devices|
+----------+------+-------+
+----------+------+-------+

// there's only 1 stateful operator and hence 0 for the index in stateOperators
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 0,
  "numRowsUpdated" : 0,
  "memoryUsedBytes" : 77
}

// Current watermark
// We've just started so it's the default start time
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

// publish 1 new record
// timestamp,device,value
// delay threshold = 10 seconds
// 1,1,1

-------------------------------------------
Batch: 1
-------------------------------------------
+----------+------+-------+
|event_time|values|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier (it's just started!) and so numRowsUpdated is 0
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 1,
  "numRowsUpdated" : 0,
  "memoryUsedBytes" : 381
}

// Current watermark
// One streaming batch has passed so it's still the default start time
// that will get changed the next streaming batch
// watermark is always one batch behind
scala> println(sq.lastProgress.eventTime.get("watermark"))
1970-01-01T00:00:00.000Z

// publish new key and old key in a single streaming batch
// new key
// 1,1
// update the already-stored key
// 0,2

-------------------------------------------
Batch: 2
-------------------------------------------
+----------+------+-------+
|event_time|values|devices|
+----------+------+-------+
+----------+------+-------+

// it's Append output mode so numRowsTotal is...FIXME
// no keys were available earlier and so numRowsUpdated is...FIXME
scala> println(sq.lastProgress.stateOperators(0).prettyJson)
{
  "numRowsTotal" : 3,
  "numRowsUpdated" : 0,
  "memoryUsedBytes" : 965
}

// Current watermark
scala> println(sq.lastProgress.eventTime.get("watermark"))
2017-08-31T10:58:49.000Z

// In the end...
sq.stop
----
