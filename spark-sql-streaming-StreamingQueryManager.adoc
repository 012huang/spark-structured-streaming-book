== [[StreamingQueryManager]] StreamingQueryManager -- Streaming Query Management

`StreamingQueryManager` is the management interface for link:spark-sql-streaming-StreamingQuery.adoc[continuous queries] (per `SparkSession`).

`StreamingQueryManager` manages all continuous structured queries per `SparkSession` that is available using `SparkSession.streams` operator.

[source, scala]
----
val spark: SparkSession = ...
val queries = spark.streams
----

[[internal-registries]]
.StreamingQueryManager's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[activeQueries]] `activeQueries`
| Registry of `StreamingQueryWrapper` per id

Used in <<active, active>>, <<get, get>>, <<startQuery, startQuery>> and <<notifyQueryTermination, notifyQueryTermination>>.
|===

=== [[notifyQueryTermination]] `notifyQueryTermination` Internal Method

CAUTION: FIXME

=== [[createQuery]] `createQuery` Internal Method

[source, scala]
----
createQuery(
  userSpecifiedName: Option[String],
  userSpecifiedCheckpointLocation: Option[String],
  df: DataFrame,
  sink: Sink,
  outputMode: OutputMode,
  useTempCheckpointLocation: Boolean,
  recoverFromCheckpointLocation: Boolean,
  trigger: Trigger,
  triggerClock: Clock): StreamingQueryWrapper
----

CAUTION: FIXME

NOTE: `userSpecifiedName` corresponds to `queryName` option (that can be defined using ``DataStreamWriter``'s link:spark-sql-streaming-DataStreamWriter.adoc#queryName[queryName] method) while `userSpecifiedCheckpointLocation` is `checkpointLocation` option.

NOTE: `createQuery` is used exclusively when `StreamingQueryManager` <<startQuery, starts executing a  streaming query>>.

=== Initialization

`StreamingQueryManager` manages the following instances:

* `StateStoreCoordinatorRef` (as `stateStoreCoordinator`)
* link:spark-sql-streaming-StreamingQueryListenerBus.adoc[StreamingQueryListenerBus] (as `listenerBus`)
* `activeQueries` which is a mutable mapping between query names and `StreamingQuery` objects.

=== [[startQuery]] Starting Execution of Streaming Query -- `startQuery` Internal Method

[source, scala]
----
startQuery(
  userSpecifiedName: Option[String],
  userSpecifiedCheckpointLocation: Option[String],
  df: DataFrame,
  sink: Sink,
  outputMode: OutputMode,
  useTempCheckpointLocation: Boolean = false,
  recoverFromCheckpointLocation: Boolean = true,
  trigger: Trigger = ProcessingTime(0),
  triggerClock: Clock = new SystemClock()): StreamingQuery
----

`startQuery` starts a link:spark-sql-streaming-StreamingQuery.adoc[streaming query].

NOTE: `trigger` defaults to `0` milliseconds (as link:spark-sql-streaming-trigger.adoc#ProcessingTime[ProcessingTime(0)]).

Internally, `startQuery` first <<createQuery, creates a streaming query>>, registers it in <<activeQueries, activeQueries>> internal registry and link:spark-sql-streaming-StreamExecution.adoc#start[starts the query].

In the end, `startQuery` returns the query (as part of the fluent API so you can chain operators) or reports the exception that was reported when starting the query.

`startQuery` reports a `IllegalArgumentException` when there is another query registered under `name`. `startQuery` looks it up in <<activeQueries, activeQueries>> internal registry.

```
Cannot start query with name [name] as a query with that name is already active
```

`startQuery` reports a `IllegalStateException` when a query is started again from checkpoint. `startQuery` looks it up in <<activeQueries, activeQueries>> internal registry.

```
Cannot start query with id [id] as another query with same id is already active.
Perhaps you are attempting to restart a query from checkpoint that is already active.
```

NOTE: `startQuery` is used exclusively when `DataStreamWriter` is link:spark-sql-streaming-DataStreamWriter.adoc#start[started].

=== [[active]] Return All Active Continuous Queries per SQLContext

[source, scala]
----
active: Array[StreamingQuery]
----

`active` method returns a collection of link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] instances for the current `SQLContext`.

=== [[get]] Getting Active Continuous Query By Name

[source, scala]
----
get(name: String): StreamingQuery
----

`get` method returns a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] by `name`.

It may throw an `IllegalArgumentException` when no StreamingQuery exists for the `name`.

```
java.lang.IllegalArgumentException: There is no active query with name hello
  at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59)
  at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59)
  at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
  at scala.collection.AbstractMap.getOrElse(Map.scala:59)
  at org.apache.spark.sql.StreamingQueryManager.get(StreamingQueryManager.scala:58)
  ... 49 elided
```

=== [[addListener]][[removeListener]] StreamingQueryListener Management - Adding or Removing Listeners

* `addListener(listener: StreamingQueryListener): Unit` adds `listener` to the internal `listenerBus`.
* `removeListener(listener: StreamingQueryListener): Unit` removes `listener` from the internal `listenerBus`.

=== [[postListenerEvent]] postListenerEvent

[source, scala]
----
postListenerEvent(event: StreamingQueryListener.Event): Unit
----

`postListenerEvent` posts a `StreamingQueryListener.Event` to `listenerBus`.

=== [[StreamingQueryListener]] StreamingQueryListener

CAUTION: FIXME

`StreamingQueryListener` is an interface for listening to query life cycle events, i.e. a query start, progress and termination events.

=== [[lastTerminatedQuery]] lastTerminatedQuery - internal barrier

CAUTION: FIXME Why is `lastTerminatedQuery` needed?

Used in:

* `awaitAnyTermination`
* `awaitAnyTermination(timeoutMs: Long)`

They all wait `10` millis before doing the check of `lastTerminatedQuery` being non-null.

It is set in:

* `resetTerminated()` resets `lastTerminatedQuery`, i.e. sets it to `null`.
* `notifyQueryTermination(terminatedQuery: StreamingQuery)` sets `lastTerminatedQuery` to be `terminatedQuery` and notifies all the threads that wait on `awaitTerminationLock`.
+
It is called from link:spark-sql-streaming-StreamExecution.adoc#runBatches[StreamExecution.runBatches].
