== [[StreamingQueryManager]] StreamingQueryManager -- Streaming Query Management

`StreamingQueryManager` is the management interface of link:spark-sql-streaming-StreamingQuery.adoc[continuous streaming queries] in a single `SparkSession`.

`StreamingQueryManager` allows for:

* <<active, Getting active structured queries>>

* <<get, Getting a structured query by id>>

* <<awaitAnyTermination, Waiting for any query to be terminated>>

* <<addListener, Registering>> or <<removeListener, de-registering>> `StreamingQueryListeners`

`StreamingQueryManager` is available using `SparkSession` and `streams` property.

[source, scala]
----
val spark: SparkSession = ...
val queries = spark.streams
----

`StreamingQueryManager` is <<creating-instance, created>> when `SessionState` is created.

.StreamingQueryManager
image::images/StreamingQueryManager.png[align="center"]

TIP: Refer to the https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-SessionState.html[Mastering Apache Spark 2] gitbook to learn about `SessionState`.

[[internal-registries]]
.StreamingQueryManager's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[activeQueries]] `activeQueries`
| Registry of `StreamingQueryWrapper` per id

Used in <<active, active>>, <<get, get>>, <<startQuery, startQuery>> and <<notifyQueryTermination, notifyQueryTermination>>.

| [[listenerBus]] `listenerBus`
a| link:spark-sql-streaming-StreamingQueryListenerBus.adoc[StreamingQueryListenerBus] (for the current <<sparkSession, SparkSession>>)

Used to:

* <<addListener, register>> or <<removeListener, deregister>> a `StreamingQueryListener`

* <<postListenerEvent, Post a streaming event>> (and notify `StreamingQueryListener` listeners about streaming events)

| [[stateStoreCoordinator]] `stateStoreCoordinator`
a| link:spark-sql-streaming-StateStoreCoordinatorRef.adoc[StateStoreCoordinatorRef] with the *StateStoreCoordinator* RPC Endpoint

* link:spark-sql-streaming-StateStoreCoordinatorRef.adoc#forDriver[Created] when `StreamingQueryManager` is <<creating-instance, created>>

Used when:

* <<notifyQueryTermination, notifyQueryTermination>>

* Stateful operators are executed, i.e. link:spark-sql-streaming-FlatMapGroupsWithStateExec.adoc#doExecute[FlatMapGroupsWithStateExec], link:spark-sql-streaming-StateStoreRestoreExec.adoc#doExecute[StateStoreRestoreExec], link:spark-sql-streaming-StateStoreSaveExec.adoc#doExecute[StateStoreSaveExec], link:spark-sql-streaming-StreamingDeduplicateExec.adoc#doExecute[StreamingDeduplicateExec] and link:spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#doExecute[StreamingSymmetricHashJoinExec]

* link:spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore[Creating StateStoreRDD (with storeUpdateFunction aborting StateStore when a task fails)]
|===

=== [[awaitAnyTermination]] `awaitAnyTermination` Method

[source, scala]
----
awaitAnyTermination(): Unit
awaitAnyTermination(timeoutMs: Long): Boolean
----

`awaitAnyTermination`...FIXME

=== [[notifyQueryTermination]] `notifyQueryTermination` Internal Method

[source, scala]
----
notifyQueryTermination(terminatedQuery: StreamingQuery): Unit
----

`notifyQueryTermination`...FIXME

NOTE: `notifyQueryTermination` is used when...FIXME

=== [[createQuery]] Creating StreamingQueryWrapper (Serializable StreamingQuery) -- `createQuery` Internal Method

[source, scala]
----
createQuery(
  userSpecifiedName: Option[String],
  userSpecifiedCheckpointLocation: Option[String],
  df: DataFrame,
  sink: Sink,
  outputMode: OutputMode,
  useTempCheckpointLocation: Boolean,
  recoverFromCheckpointLocation: Boolean,
  trigger: Trigger,
  triggerClock: Clock): StreamingQueryWrapper
----

`createQuery`

[NOTE]
====
`recoverFromCheckpointLocation` flag corresponds to `recoverFromCheckpointLocation` flag that `StreamingQueryManager` uses to <<startQuery, start a streaming query>> and which is enabled by default (and is in fact the only place where `createQuery` is used).

* `memory` sink has the flag enabled for link:spark-sql-streaming-OutputMode.adoc#Complete[Complete] output mode only

* `foreach` sink has the flag always enabled

* `console` sink has the flag always disabled

* all other sinks have the flag always enabled
====

NOTE: `userSpecifiedName` corresponds to `queryName` option (that can be defined using ``DataStreamWriter``'s link:spark-sql-streaming-DataStreamWriter.adoc#queryName[queryName] method) while `userSpecifiedCheckpointLocation` is `checkpointLocation` option.

NOTE: `createQuery` is used exclusively when `StreamingQueryManager` <<startQuery, starts executing a streaming query>>.

=== Initialization

`StreamingQueryManager` manages the following instances:

* link:spark-sql-streaming-StateStoreCoordinatorRef.adoc[StateStoreCoordinatorRef] (as `stateStoreCoordinator`)
* link:spark-sql-streaming-StreamingQueryListenerBus.adoc[StreamingQueryListenerBus] (as `listenerBus`)
* `activeQueries` which is a mutable mapping between query names and `StreamingQuery` objects.

=== [[startQuery]] Starting Execution of Streaming Query -- `startQuery` Internal Method

[source, scala]
----
startQuery(
  userSpecifiedName: Option[String],
  userSpecifiedCheckpointLocation: Option[String],
  df: DataFrame,
  sink: Sink,
  outputMode: OutputMode,
  useTempCheckpointLocation: Boolean = false,
  recoverFromCheckpointLocation: Boolean = true,
  trigger: Trigger = ProcessingTime(0),
  triggerClock: Clock = new SystemClock()): StreamingQuery
----

`startQuery` starts a link:spark-sql-streaming-StreamingQuery.adoc[streaming query].

NOTE: `trigger` defaults to `0` milliseconds (as link:spark-sql-streaming-Trigger.adoc#ProcessingTime[ProcessingTime(0)]).

Internally, `startQuery` first <<createQuery, creates a streaming query>>, registers it in <<activeQueries, activeQueries>> internal registry and link:spark-sql-streaming-StreamExecution.adoc#start[starts the query].

In the end, `startQuery` returns the query (as part of the fluent API so you can chain operators) or reports the exception that was reported when starting the query.

`startQuery` reports a `IllegalArgumentException` when there is another query registered under `name`. `startQuery` looks it up in <<activeQueries, activeQueries>> internal registry.

```
Cannot start query with name [name] as a query with that name is already active
```

`startQuery` reports a `IllegalStateException` when a query is started again from checkpoint. `startQuery` looks it up in <<activeQueries, activeQueries>> internal registry.

```
Cannot start query with id [id] as another query with same id is already active.
Perhaps you are attempting to restart a query from checkpoint that is already active.
```

NOTE: `startQuery` is used exclusively when `DataStreamWriter` is link:spark-sql-streaming-DataStreamWriter.adoc#start[started].

=== [[active]] Getting All Active Streaming Queries -- `active` Method

[source, scala]
----
active: Array[StreamingQuery]
----

`active` gets <<activeQueries, active queries>>.

=== [[get]] Getting Active Continuous Query By Name -- `get` Method

[source, scala]
----
get(name: String): StreamingQuery
----

`get` method returns a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] by `name`.

It may throw an `IllegalArgumentException` when no StreamingQuery exists for the `name`.

```
java.lang.IllegalArgumentException: There is no active query with name hello
  at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59)
  at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59)
  at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
  at scala.collection.AbstractMap.getOrElse(Map.scala:59)
  at org.apache.spark.sql.StreamingQueryManager.get(StreamingQueryManager.scala:58)
  ... 49 elided
```

=== [[lastTerminatedQuery]] `lastTerminatedQuery` Internal Barrier

CAUTION: FIXME Why is `lastTerminatedQuery` needed?

Used in:

* `awaitAnyTermination`
* `awaitAnyTermination(timeoutMs: Long)`

They all wait `10` millis before doing the check of `lastTerminatedQuery` being non-null.

It is set in:

* `resetTerminated()` resets `lastTerminatedQuery`, i.e. sets it to `null`.
* `notifyQueryTermination(terminatedQuery: StreamingQuery)` sets `lastTerminatedQuery` to be `terminatedQuery` and notifies all the threads that wait on `awaitTerminationLock`.
+
It is called from link:spark-sql-streaming-StreamExecution.adoc#runBatches[StreamExecution.runBatches].

=== [[postListenerEvent]] Posting StreamingQueryListener Event to StreamingQueryListenerBus -- `postListenerEvent` Method

[source, scala]
----
postListenerEvent(event: StreamingQueryListener.Event): Unit
----

`postListenerEvent` simply posts the input `event` to <<listenerBus, StreamingQueryListenerBus>> (in the current `SparkSession`).

.StreamingQueryManager Propagates StreamingQueryListener Events
image::images/StreamingQueryManager-postListenerEvent.png[align="center"]

NOTE: `postListenerEvent` is used exclusively when `StreamExecution` link:spark-sql-streaming-StreamExecution.adoc#postEvent[posts a streaming event].

=== [[creating-instance]] Creating StreamingQueryManager Instance

`StreamingQueryManager` takes the following when created:

* [[sparkSession]] `SparkSession`

`StreamingQueryManager` initializes the <<internal-registries, internal registries and counters>>.

=== [[addListener]] Registering StreamingQueryListener -- `addListener` Method

[source, scala]
----
addListener(listener: StreamingQueryListener): Unit
----

`addListener` requests <<listenerBus, StreamingQueryListenerBus>> to link:spark-sql-streaming-StreamingQueryListenerBus.adoc#addListener[add] the input `listener`.

=== [[removeListener]] De-Registering StreamingQueryListener -- `removeListener` Method

[source, scala]
----
removeListener(listener: StreamingQueryListener): Unit
----

`removeListener` requests <<listenerBus, StreamingQueryListenerBus>> to link:spark-sql-streaming-StreamingQueryListenerBus.adoc#removeListener[remove] the input `listener`.
