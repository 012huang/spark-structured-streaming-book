== [[StreamingDeduplicateExec]] StreamingDeduplicateExec -- Unary Physical Operator for Streaming Deduplication

`StreamingDeduplicateExec` is a unary physical operator (i.e. `UnaryExecNode`) that link:spark-sql-streaming-StateStoreWriter.adoc[writes state to StateStore] with link:spark-sql-streaming-WatermarkSupport.adoc[support for streaming watermark].

`StreamingDeduplicateExec` is <<creating-instance, created>> exclusively when `StreamingDeduplicationStrategy` link:spark-sql-streaming-StreamingDeduplicationStrategy.adoc#apply[plans Deduplicate unary logical operators].

.StreamingDeduplicateExec and StreamingDeduplicationStrategy
image::images/StreamingDeduplicateExec-StreamingDeduplicationStrategy.png[align="center"]

[source, scala]
----
val uniqueValues = spark.
  readStream.
  format("rate").
  load.
  dropDuplicates("value")  // <-- creates Deduplicate logical operator

scala> println(uniqueValues.queryExecution.logical.numberedTreeString)
00 Deduplicate [value#214L], true
01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#213, value#214L]

scala> uniqueValues.explain
== Physical Plan ==
StreamingDeduplicate [value#214L], StatefulOperatorStateInfo(<unknown>,5a65879c-67bc-4e77-b417-6100db6a52a2,0,0), 0
+- Exchange hashpartitioning(value#214L, 200)
   +- StreamingRelation rate, [timestamp#213, value#214L]

// Start the query and hence StreamingDeduplicateExec
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val sq = uniqueValues.
  writeStream.
  format("console").
  option("truncate", false).
  trigger(Trigger.ProcessingTime(10.seconds)).
  outputMode(OutputMode.Update).
  start

// sorting not supported for non-aggregate queries
// and so values are unsorted

-------------------------------------------
Batch: 0
-------------------------------------------
+---------+-----+
|timestamp|value|
+---------+-----+
+---------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2017-07-25 22:12:03.018|0    |
|2017-07-25 22:12:08.018|5    |
|2017-07-25 22:12:04.018|1    |
|2017-07-25 22:12:06.018|3    |
|2017-07-25 22:12:05.018|2    |
|2017-07-25 22:12:07.018|4    |
+-----------------------+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2017-07-25 22:12:10.018|7    |
|2017-07-25 22:12:09.018|6    |
|2017-07-25 22:12:12.018|9    |
|2017-07-25 22:12:13.018|10   |
|2017-07-25 22:12:15.018|12   |
|2017-07-25 22:12:11.018|8    |
|2017-07-25 22:12:14.018|11   |
|2017-07-25 22:12:16.018|13   |
|2017-07-25 22:12:17.018|14   |
|2017-07-25 22:12:18.018|15   |
+-----------------------+-----+

// Eventually...
sq.stop
----

[[metrics]]
.StreamingDeduplicateExec's SQLMetrics
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[allUpdatesTimeMs]] `allUpdatesTimeMs`
|

| [[allRemovalsTimeMs]] `allRemovalsTimeMs`
|

| [[commitTimeMs]] `commitTimeMs`
|

| [[numTotalStateRows]] `numTotalStateRows`
| Number of keys in the link:spark-sql-streaming-StateStore.adoc[StateStore]

| [[numOutputRows]] `numOutputRows`
|

| [[numTotalStateRows]] `numTotalStateRows`
| Number of keys in the link:spark-sql-streaming-StateStore.adoc[StateStore]

| [[numUpdatedStateRows]] `numUpdatedStateRows`
|

| [[stateMemory]] `stateMemory`
| Memory used by the link:spark-sql-streaming-StateStore.adoc[StateStore]
|===

.StreamingDeduplicateExec in web UI (Details for Query)
image::images/StreamingDeduplicateExec-webui-query-details.png[align="center"]

[[output]]
The output schema of `StreamingDeduplicateExec` is exactly the <<child, child>>'s output schema.

[[outputPartitioning]]
The output partitioning of `StreamingDeduplicateExec` is exactly the <<child, child>>'s output partitioning.

=== [[doExecute]] Executing StreamingDeduplicateExec -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is a part of `SparkPlan` contract to produce the result of a physical operator as an RDD of internal binary rows (i.e. `InternalRow`).

Internally, `doExecute` initializes link:spark-sql-streaming-StateStoreWriter.adoc#metrics[metrics].

`doExecute` executes <<child, child>> physical operator and link:spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore[creates a StateStoreRDD] with `storeUpdateFunction` that:

1. Generates an unsafe projection to access the key field (using <<keyExpressions, keyExpressions>> and the output schema of <<child, child>>).

1. Filters out rows from `Iterator[InternalRow]` that match `watermarkPredicateForData` (when defined and <<timeoutConf, timeoutConf>> is `EventTimeTimeout`)

1. For every row (as `InternalRow`)

* Extracts the key from the row (using the unsafe projection above)

* link:spark-sql-streaming-StateStore.adoc#get[Gets the saved state] in `StateStore` for the key

* (when there was a state for the key in the row) Filters out (aka _drops_) the row

* (when there was _no_ state for the key in the row) Stores a new (and empty) state for the key and increments <<numUpdatedStateRows, numUpdatedStateRows>> and <<numOutputRows, numOutputRows>> metrics.

1. In the end, `storeUpdateFunction` creates a `CompletionIterator` that executes a completion function (aka `completionFunction`) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows).
+
The completion function does the following:

* Updates <<allUpdatesTimeMs, allUpdatesTimeMs>> metric (that is the total time to execute `storeUpdateFunction`)

* Updates <<allRemovalsTimeMs, allRemovalsTimeMs>> metric with the time taken to link:spark-sql-streaming-WatermarkSupport.adoc#removeKeysOlderThanWatermark[remove keys older than the watermark from the StateStore]

* Updates <<commitTimeMs, commitTimeMs>> metric with the time taken to link:spark-sql-streaming-StateStore.adoc#commit[commit the changes to the StateStore]

* link:spark-sql-streaming-StateStoreWriter.adoc#setStoreMetrics[Sets StateStore-specific metrics]

=== [[creating-instance]] Creating StreamingDeduplicateExec Instance

`StreamingDeduplicateExec` takes the following when created:

* [[keyExpressions]] Attributes for key
* [[child]] Child physical plan (i.e. `SparkPlan`)
* [[stateInfo]] Optional `StatefulOperatorStateInfo`
* [[eventTimeWatermark]] Optional event time watermark
